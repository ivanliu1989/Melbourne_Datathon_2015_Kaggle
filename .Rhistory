train$flag_class <- ifelse(train$flag_class == 'Y', 1, -1)
validation$flag_class <- ifelse(validation$flag_class == 'Y', 1, -1)
feat <- c(3:76)
head(train[, feat])
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
rm(list = ls()); gc()
library(RSofia);library(pROC);library(caret)
load('data/9_train_validation_test_20151108_feat.RData');ls()
### Center Scale
feat <- c(3:76) #3:59
prep <- preProcess(train[, feat], method = c('center',"scale"), verbose =T)
train[, feat] <- predict(prep, train[, feat])
validation[, feat] <- predict(prep, validation[, feat])
train$flag_class <- ifelse(train$flag_class == 'Y', 1, -1)
validation$flag_class <- ifelse(validation$flag_class == 'Y', 1, -1)
feat <- c(3:77
)
fit <- sofia(flag_class ~ ., data=train[,feat], lambda = 1e-3, iiterations = 1e+25, random_seed = 13560,
learner_type = 'logreg-pegasos', #c("pegasos", "sgd-svm","passive-aggressive", "margin-perceptron", "romma", "logreg-pegasos"),
eta_type = 'pegasos', #c("pegasos", "basic", "constant"),
loop_type = 'balanced-stochastic', #c("stochastic","balanced-stochastic", "rank", "roc", "query-norm-rank","combined-ranking", "combined-roc"),
rank_step_probability = 0.5,
passive_aggressive_c = 1e+07, passive_aggressive_lambda = 1e+1, dimensionality = 60,
perceptron_margin_size = 1, training_objective = F, hash_mask_bits = 0,
verbose = T, reserve = 1
)
#--------------------basic prediction using sofia--------------
val <- validation
p <- predict(fit, newdata=val[,feat], prediction_type = "logistic") #logistic linear
# p[is.na(p)] <- 1
val$Y <- p
tot_invest <- aggregate(INVEST ~ ACCOUNT_ID,data=val, sum, na.rm=T); names(tot_invest) <- c('ACCOUNT_ID', 'TOT_INVEST')
val <- merge(val, tot_invest, all.x = TRUE, all.y = FALSE, by = c('ACCOUNT_ID'))
val$INVEST_PERCENT <- val$INVEST/val$TOT_INVEST * val$Y#(val$Y - 0.5) * 2
pred_fin <- aggregate(INVEST_PERCENT ~ ACCOUNT_ID, data=val, mean, na.rm=F)
pred_fin2 <- aggregate(Y ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
### Model Performance
v <- merge(val_fin,pred_fin,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
v <- merge(v,pred_fin2,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$INVEST_PERCENT);print(auc(rocobj)) # Invest * Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$Y);print(auc(rocobj)) # Average Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(xgboost);library(pROC);library(caret)
load('data/9_train_validation_test_20151108_feat.RData');ls()
# c(101183757,101183885,101184013) - last 3 event
# c(101150834,101153072,101149398) - validation
# c(101093076,101093194,101093312)
# c(101128387,101150348,101152275)
# c(101149870,101150716,101153308)
### Test
# train <- total
### Validation
training <- train#[!train$EVENT_ID %in% c(101149870,101150716,101153308),]
testing <- train[train$EVENT_ID %in% c(101149870,101150716,101153308),]
dim(training); dim(testing)
training$flag_class <- ifelse(training$flag_class == 'Y', 1, 0)
colnames(training)
feat <- colnames(training)[c(3:76)]
bst <-
xgboost(
data = as.matrix(training[,feat]), label = training$flag_class, max.depth = 6, eta = 0.15, nround = 500, maximize = F, #500,0.15
nthread = 4, objective = "binary:logistic", verbose = 1, early.stop.round = 10, print.every.n = 10, metrics = 'auc'
)
val <- validation
# val <- testing
p <- predict(bst, as.matrix(val[,feat]))
# p <- predict(bst, as.matrix(train[,feat]))
val$Y <- p
tot_invest <- aggregate(INVEST ~ ACCOUNT_ID,data=val, sum, na.rm=T); names(tot_invest) <- c('ACCOUNT_ID', 'TOT_INVEST')
val <- merge(val, tot_invest, all.x = TRUE, all.y = FALSE, by = c('ACCOUNT_ID'))
val$INVEST_PERCENT <- val$INVEST/val$TOT_INVEST * val$Y#(val$Y - 0.5) * 2
pred_fin <- aggregate(INVEST_PERCENT ~ ACCOUNT_ID, data=val, mean, na.rm=F)
pred_fin2 <- aggregate(Y ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
#########################
### Model Performance ###
#########################
v <- merge(val_fin,pred_fin,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
v <- merge(v,pred_fin2,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$INVEST_PERCENT);print(auc(rocobj)) # Invest * Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$Y);print(auc(rocobj)) # Average Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
val <- testing
p <- predict(bst, as.matrix(val[,feat]))
# p <- predict(bst, as.matrix(train[,feat]))
val$Y <- p
tot_invest <- aggregate(INVEST ~ ACCOUNT_ID,data=val, sum, na.rm=T); names(tot_invest) <- c('ACCOUNT_ID', 'TOT_INVEST')
val <- merge(val, tot_invest, all.x = TRUE, all.y = FALSE, by = c('ACCOUNT_ID'))
val$INVEST_PERCENT <- val$INVEST/val$TOT_INVEST * val$Y#(val$Y - 0.5) * 2
pred_fin <- aggregate(INVEST_PERCENT ~ ACCOUNT_ID, data=val, mean, na.rm=F)
pred_fin2 <- aggregate(Y ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
#########################
### Model Performance ###
#########################
v <- merge(val_fin,pred_fin,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
v <- merge(v,pred_fin2,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$INVEST_PERCENT);print(auc(rocobj)) # Invest * Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$Y);print(auc(rocobj)) # Average Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
names <- dimnames(as.matrix(train[,feat]))[[2]]
importance_matrix <- xgb.importance(names, model = bst)
xgb.plot.importance(importance_matrix)
importance_matrix
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(xgboost);library(pROC);library(caret)
load('data/9_train_validation_test_20151108_feat.RData');ls()
all <- rbind(total, test); str(all)
total$flag_class <- ifelse(total$flag_class == 'Y', 1, 0)
names(total)
total$flag_class <- ifelse(total$flag_class == 'Y', 1, 0)
feat <- colnames(total)[c(3:56, 59)]
bst <-
xgboost(
data = as.matrix(total[,feat]), label = total$flag_class, max.depth = 6, eta = 0.15, nround = 500,
nthread = 4, objective = "binary:logistic", verbose = 0, metrics = 'auc'
)
p <- predict(bst, as.matrix(all[,feat]))
head(p)
p
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(xgboost);library(pROC);library(caret)
load('data/9_train_validation_test_20151108_feat.RData');ls()
all <- rbind(total, test); str(all)
###################################
# 1. xgboost meta features ########
###################################
total$flag_class <- ifelse(total$flag_class == 'Y', 1, 0)
# gbm - raw
feat <- colnames(total)[c(3:56, 59)]
bst <-
xgboost(
data = as.matrix(total[,feat]), label = total$flag_class, max.depth = 6, eta = 0.15, nround = 500,
nthread = 4, objective = "binary:logistic", verbose = 0, metrics = 'auc'
)
p <- predict(bst, as.matrix(all[,feat]))
head(p)
p
all$xgb_gbm_meta_raw <- p
bst <-
xgboost(
data = as.matrix(total[,feat2]), label = total$flag_class, max.depth = 6, eta = 0.15, nround = 500,
nthread = 4, objective = "binary:logistic", verbose = 0, metrics = 'auc'
)
p <- predict(bst, as.matrix(all[,feat2]))
all$xgb_gbm_meta_new <- p
feat <- colnames(total)[c(3:56, 59)]
feat2 <- colnames(total)[c(3:76)]
bst <-
xgboost(
data = as.matrix(total[,feat2]), label = total$flag_class, max.depth = 6, eta = 0.15, nround = 500,
nthread = 4, objective = "binary:logistic", verbose = 0, metrics = 'auc'
)
p <- predict(bst, as.matrix(all[,feat2]))
all$xgb_gbm_meta_new <- p
head(all)
# rf - old
bst <-
xgboost(
data = as.matrix(total[,feat]), label = total$flag_class, max.depth = 9, num_parallel_tree = 150, subsample = 0.5, colsample_bytree =
0.5, nround = 1, objective = "binary:logistic"
)
p <- predict(bst, as.matrix(all[,feat]))
all$xgb_rf_meta_raw <- p
# rf - new
bst <-
xgboost(
data = as.matrix(total[,feat2]), label = total$flag_class, max.depth = 9, num_parallel_tree = 150, subsample = 0.5, colsample_bytree =
0.5, nround = 1, objective = "binary:logistic"
)
p <- predict(bst, as.matrix(all[,feat2]))
all$xgb_rf_meta_new <- p
head(all)
tail(all)
save(all, file='data/9_train_validation_test_20151108_meta_xgb_v1.RData')
library(h2o)
localH2O <- h2o.init(ip = 'localhost', port = 54321, max_mem_size = '12g')
total$flag_class <- as.factor(total$flag_class); levels(total$flag_class) <- c(0,1)
train_df <- as.h2o(localH2O, total) # train
test_df <- as.h2o(localH2O, all) # test
independent1 <- c(colnames(train_df[,feat]))
independent2 <- c(colnames(train_df[,feat2]))
dependent <- "flag_class"
fit <- h2o.gbm(
y = dependent, x = independent1, data = train_df, #train_df | total_df
n.trees = 200, interaction.depth = 8, n.minobsinnode = 1,
shrinkage = 0.25, distribution = "bernoulli", n.bins = 20,  #AUTO
importance = F
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
all$h2o_gbm_meta_raw <- as.vector(p$X1)
fit <- h2o.gbm(
y = dependent, x = independent2, data = train_df, #train_df | total_df
n.trees = 200, interaction.depth = 8, n.minobsinnode = 1,
shrinkage = 0.25, distribution = "bernoulli", n.bins = 20,  #AUTO
importance = F
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
all$h2o_gbm_meta_new <- as.vector(p$X1)
head(all)
fit <-
h2o.deeplearning(
y = dependent, x = independent1, data = train_df, classification = T,
activation = "RectifierWithDropout",#TanhWithDropout "RectifierWithDropout" nfolds = 5,
hidden = c(64,32), adaptive_rate = T, rho = 0.99,
epsilon = 1e-4, rate = 0.01, rate_decay = 0.9, # rate_annealing = ,
momentum_start = 0.5, momentum_stable = 0.99, # momentum_ramp
nesterov_accelerated_gradient = F, input_dropout_ratio = 0.5, hidden_dropout_ratios = c(0.5,0.5),
l2 = 3e-6, max_w2 = 4, #Rect
loss = 'CrossEntropy', classification_stop = -1,
diagnostics = T, variable_importances = T, ignore_const_cols = T,
force_load_balance = T, replicate_training_data = T, shuffle_training_data = T,
sparse = F, epochs = 300 #, reproducible, score_validation_sampling seed = 8,
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
all$h2o_dl_meta_raw <- as.vector(p$X1)
head(all)
head(all, 50)
fit <-
h2o.deeplearning(
y = dependent, x = independent1, data = train_df, classification = T,
activation = "RectifierWithDropout",#TanhWithDropout "RectifierWithDropout" nfolds = 5,
hidden = c(64,32), adaptive_rate = T, rho = 0.99,
epsilon = 1e-4, rate = 0.01, rate_decay = 0.9, # rate_annealing = ,
momentum_start = 0.5, momentum_stable = 0.99, # momentum_ramp
nesterov_accelerated_gradient = F, input_dropout_ratio = 0.5, hidden_dropout_ratios = c(0.5,0.5),
l2 = 3e-6, max_w2 = 4, #Rect
loss = 'CrossEntropy', classification_stop = -1,
diagnostics = T, variable_importances = T, ignore_const_cols = T,
force_load_balance = T, replicate_training_data = T, shuffle_training_data = T,
sparse = F, epochs = 300 #, reproducible, score_validation_sampling seed = 8,
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
all$h2o_dl_meta_raw <- as.vector(p$X1)
head(all, 50)
fit <-
h2o.deeplearning(
y = dependent, x = independent1, data = train_df, classification = T,
activation = "RectifierWithDropout",#TanhWithDropout "RectifierWithDropout" nfolds = 5,
hidden = c(64,32), adaptive_rate = T, rho = 0.99,
epsilon = 1e-4, rate = 0.01, rate_decay = 0.9, # rate_annealing = ,
momentum_start = 0.5, momentum_stable = 0.99, # momentum_ramp
nesterov_accelerated_gradient = F, input_dropout_ratio = 0.5, hidden_dropout_ratios = c(0.5,0.5),
l2 = 3e-6, max_w2 = 4, #Rect
loss = 'CrossEntropy', classification_stop = -1,
diagnostics = T, variable_importances = T, ignore_const_cols = T,
force_load_balance = T, replicate_training_data = T, shuffle_training_data = T,
sparse = F, epochs = 300 #, reproducible, score_validation_sampling seed = 8,
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
p
head(p, 50)
fit <-
h2o.deeplearning(
y = dependent, x = independent1, data = train_df, classification = T,
activation = "RectifierWithDropout",#TanhWithDropout "RectifierWithDropout" nfolds = 5,
hidden = c(256,256,256), adaptive_rate = T, rho = 0.99,
epsilon = 1e-4, rate = 0.01, rate_decay = 0.9, # rate_annealing = ,
momentum_start = 0.5, momentum_stable = 0.99, # momentum_ramp
nesterov_accelerated_gradient = F, input_dropout_ratio = 0.5, hidden_dropout_ratios = c(0.5,0.5,0.5),
l2 = 3e-6, max_w2 = 4, #Rect
loss = 'CrossEntropy', classification_stop = -1,
diagnostics = T, variable_importances = T, ignore_const_cols = T,
force_load_balance = T, replicate_training_data = T, shuffle_training_data = T,
sparse = F, epochs = 9 #, reproducible, score_validation_sampling seed = 8,
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
p
head(p, 50)
head(all, 50)
fit <-
h2o.deeplearning(
y = dependent, x = independent1, data = train_df, classification = T,
activation = "RectifierWithDropout",#TanhWithDropout "RectifierWithDropout" nfolds = 5,
hidden = c(256,128,64), adaptive_rate = T, rho = 0.99,
epsilon = 1e-4, rate = 0.01, rate_decay = 0.9, # rate_annealing = ,
momentum_start = 0.5, momentum_stable = 0.99, # momentum_ramp
nesterov_accelerated_gradient = F, input_dropout_ratio = 0.5, hidden_dropout_ratios = c(0.5,0.5,0.5),
l2 = 3e-6, max_w2 = 4, #Rect
loss = 'CrossEntropy', classification_stop = -1,
diagnostics = T, variable_importances = T, ignore_const_cols = T,
force_load_balance = T, replicate_training_data = T, shuffle_training_data = T,
sparse = F, epochs = 100 #, reproducible, score_validation_sampling seed = 8,
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
p
head(p)
head(p, 50)
head(all, 50)
all$h2o_dl_meta_raw <- as.vector(p$X1)
head(all, 50)
fit <-
h2o.deeplearning(
y = dependent, x = independent1, data = train_df, classification = T,
activation = "RectifierWithDropout",#TanhWithDropout "RectifierWithDropout" nfolds = 5,
hidden = c(64,32), adaptive_rate = T, rho = 0.99,
epsilon = 1e-4, rate = 0.01, rate_decay = 0.9, # rate_annealing = ,
momentum_start = 0.5, momentum_stable = 0.99, # momentum_ramp
nesterov_accelerated_gradient = F, input_dropout_ratio = 0.5, hidden_dropout_ratios = c(0.5,0.5),
l2 = 3e-6, max_w2 = 4, #Rect
loss = 'CrossEntropy', classification_stop = -1,
diagnostics = T, variable_importances = T, ignore_const_cols = T,
force_load_balance = T, replicate_training_data = T, shuffle_training_data = T,
sparse = F, epochs = 100 #, reproducible, score_validation_sampling seed = 8,
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
head(p)
p
head(all, 50)
head(p, 50)
all$h2o_dl_meta_raw <- as.vector(p$X1)
fit <-
h2o.deeplearning(
y = dependent, x = independent2, data = train_df, classification = T,
activation = "RectifierWithDropout",#TanhWithDropout "RectifierWithDropout" nfolds = 5,
hidden = c(64,32), adaptive_rate = T, rho = 0.99,
epsilon = 1e-4, rate = 0.01, rate_decay = 0.9, # rate_annealing = ,
momentum_start = 0.5, momentum_stable = 0.99, # momentum_ramp
nesterov_accelerated_gradient = F, input_dropout_ratio = 0.5, hidden_dropout_ratios = c(0.5,0.5),
l2 = 3e-6, max_w2 = 4, #Rect
loss = 'CrossEntropy', classification_stop = -1,
diagnostics = T, variable_importances = T, ignore_const_cols = T,
force_load_balance = T, replicate_training_data = T, shuffle_training_data = T,
sparse = F, epochs = 300 #, reproducible, score_validation_sampling seed = 8,
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
all$h2o_dl_meta_new <- as.vector(p$X1)
head(all, 50)
fit <-
h2o.randomForest(
y = dependent, x = independent1, data = train_df, #train_df | total_df #validation_frame
ntree = 100, depth = 10, mtries = 8, sample.rate = 0.8, nbins = 10, importance = F
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
all$h2o_rf_meta_raw <- as.vector(p$X1)
fit <-
h2o.randomForest(
y = dependent, x = independent2, data = train_df, #train_df | total_df #validation_frame
ntree = 100, depth = 10, mtries = 8, sample.rate = 0.8, nbins = 10, importance = F
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
all$h2o_rf_meta_new <- as.vector(p$X1)
head(all, 50)
fit <-
h2o.glm(
y = dependent, x = independent1, data = train_df, #train_df | total_df
family = 'binomial', link = 'logit',alpha = 0.5, # 1 lasso 0 ridge
lambda = 1e-01, lambda_search = T, nlambda = 55, lambda.min.ratio = 1e-08,
strong_rules = T, standardize = T, intercept = T, use_all_factor_levels = T,
epsilon = 1e-4, iter.max = 900, higher_accuracy = T, disable_line_search = F
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
all$h2o_glm_meta <- as.vector(p$X1)
fit <-
h2o.glm(
y = dependent, x = independent2, data = train_df, #train_df | total_df
family = 'binomial', link = 'logit',alpha = 0.5, # 1 lasso 0 ridge
lambda = 1e-01, lambda_search = T, nlambda = 55, lambda.min.ratio = 1e-08,
strong_rules = T, standardize = T, intercept = T, use_all_factor_levels = T,
epsilon = 1e-4, iter.max = 900, higher_accuracy = T, disable_line_search = F
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
all$h2o_glm_meta <- as.vector(p$X1)
feat
feat2
head(all)
all$h2o_glm_meta <- NULL
fit <-
h2o.glm(
y = dependent, x = independent1, data = train_df, #train_df | total_df
family = 'binomial', link = 'logit',alpha = 0.5, # 1 lasso 0 ridge
lambda = 1e-01, lambda_search = T, nlambda = 55, lambda.min.ratio = 1e-08,
strong_rules = T, standardize = T, intercept = T, use_all_factor_levels = T,
epsilon = 1e-4, iter.max = 900, higher_accuracy = T, disable_line_search = F
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
all$h2o_glm_meta_raw <- as.vector(p$X1)
fit <-
h2o.glm(
y = dependent, x = independent2, data = train_df, #train_df | total_df
family = 'binomial', link = 'logit',alpha = 0.5, # 1 lasso 0 ridge
lambda = 1e-01, lambda_search = T, nlambda = 55, lambda.min.ratio = 1e-08,
strong_rules = T, standardize = T, intercept = T, use_all_factor_levels = T,
epsilon = 1e-4, iter.max = 900, higher_accuracy = T, disable_line_search = F
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
all$h2o_glm_meta_new <- as.vector(p$X1)
head(all)
save(all, file='data/9_train_validation_test_20151108_meta_h2o_v2.RData')
head(total[, feat])
names(total)
length(p)
dim(p)
dim(all)
dim(test_df)
library(RSofia);library(caret)
### Center Scale
prep <- preProcess(total[, feat], method = c('center',"scale"), verbose =T)
tot <- total
tot[, feat] <- predict(prep, tot[, feat])
tot$flag_class <- ifelse(tot$flag_class == 'Y', 1, -1)
feat_sofia <- c(feat, 77)
dim(tot)
prep <- preProcess(all[, feat], method = c('center',"scale"), verbose =T)
tot <- all
tot[, feat] <- predict(prep, tot[, feat])
tot$flag_class <- ifelse(tot$flag_class == 'Y', 1, -1)
feat_sofia <- c(feat, 77)
dim(tot)
fit <- sofia(flag_class ~ ., data=tot[,feat_sofia], lambda = 1e-3, iiterations = 1e+25, random_seed = 13560,
learner_type = 'logreg-pegasos', #c("pegasos", "sgd-svm","passive-aggressive", "margin-perceptron", "romma", "logreg-pegasos"),
eta_type = 'pegasos', #c("pegasos", "basic", "constant"),
loop_type = 'balanced-stochastic', #c("stochastic","balanced-stochastic", "rank", "roc", "query-norm-rank","combined-ranking", "combined-roc"),
rank_step_probability = 0.5,
passive_aggressive_c = 1e+07, passive_aggressive_lambda = 1e+1, dimensionality = 60,
perceptron_margin_size = 1, training_objective = F, hash_mask_bits = 0,
verbose = T, reserve = 1
)
p <- predict(fit, newdata=tot[,feat_sofia], prediction_type = "logistic")
all$sofia_bal_meta_raw <- as.vector(p)
fit <- sofia(flag_class ~ ., data=tot[,feat_sofia], lambda = 1e-3, iiterations = 1e+25, random_seed = 13560,
learner_type = 'logreg-pegasos', #c("pegasos", "sgd-svm","passive-aggressive", "margin-perceptron", "romma", "logreg-pegasos"),
eta_type = 'pegasos', #c("pegasos", "basic", "constant"),
loop_type = 'combined-roc', #c("stochastic","balanced-stochastic", "rank", "roc", "query-norm-rank","combined-ranking", "combined-roc"),
rank_step_probability = 0.5,
passive_aggressive_c = 1e+07, passive_aggressive_lambda = 1e+1, dimensionality = 60,
perceptron_margin_size = 1, training_objective = F, hash_mask_bits = 0,
verbose = T, reserve = 1
)
p <- predict(fit, newdata=tot[feat_sofia], prediction_type = "logistic")
total$sofia_roc_meta_raw <- as.vector(p)
feat_sofia <- c(feat, 77)
fit <- sofia(flag_class ~ ., data=tot[,feat_sofia], lambda = 1e-3, iiterations = 1e+25, random_seed = 13560,
learner_type = 'logreg-pegasos', #c("pegasos", "sgd-svm","passive-aggressive", "margin-perceptron", "romma", "logreg-pegasos"),
eta_type = 'pegasos', #c("pegasos", "basic", "constant"),
loop_type = 'balanced-stochastic', #c("stochastic","balanced-stochastic", "rank", "roc", "query-norm-rank","combined-ranking", "combined-roc"),
rank_step_probability = 0.5,
passive_aggressive_c = 1e+07, passive_aggressive_lambda = 1e+1, dimensionality = 60,
perceptron_margin_size = 1, training_objective = F, hash_mask_bits = 0,
verbose = T, reserve = 1
)
√
feat_sofia
feat_sofia <- c(feat, 'flag_class')
fit <- sofia(flag_class ~ ., data=tot[,feat_sofia], lambda = 1e-3, iiterations = 1e+25, random_seed = 13560,
learner_type = 'logreg-pegasos', #c("pegasos", "sgd-svm","passive-aggressive", "margin-perceptron", "romma", "logreg-pegasos"),
eta_type = 'pegasos', #c("pegasos", "basic", "constant"),
loop_type = 'balanced-stochastic', #c("stochastic","balanced-stochastic", "rank", "roc", "query-norm-rank","combined-ranking", "combined-roc"),
rank_step_probability = 0.5,
passive_aggressive_c = 1e+07, passive_aggressive_lambda = 1e+1, dimensionality = 60,
perceptron_margin_size = 1, training_objective = F, hash_mask_bits = 0,
verbose = T, reserve = 1
)
p <- predict(fit, newdata=tot[,feat_sofia], prediction_type = "logistic")
all$sofia_bal_meta_raw <- as.vector(p)
fit <- sofia(flag_class ~ ., data=tot[,feat_sofia], lambda = 1e-3, iiterations = 1e+25, random_seed = 13560,
learner_type = 'logreg-pegasos', #c("pegasos", "sgd-svm","passive-aggressive", "margin-perceptron", "romma", "logreg-pegasos"),
eta_type = 'pegasos', #c("pegasos", "basic", "constant"),
loop_type = 'combined-roc', #c("stochastic","balanced-stochastic", "rank", "roc", "query-norm-rank","combined-ranking", "combined-roc"),
rank_step_probability = 0.5,
passive_aggressive_c = 1e+07, passive_aggressive_lambda = 1e+1, dimensionality = 60,
perceptron_margin_size = 1, training_objective = F, hash_mask_bits = 0,
verbose = T, reserve = 1
)
p <- predict(fit, newdata=tot[feat_sofia], prediction_type = "logistic")
total$sofia_roc_meta_raw <- as.vector(p)
all$sofia_roc_meta_raw <- as.vector(p)
head(all)
prep <- preProcess(all[, feat2], method = c('center',"scale"), verbose =T)
tot <- all
tot[, feat2] <- predict(prep, tot[, feat2])
tot$flag_class <- ifelse(tot$flag_class == 'Y', 1, -1)
feat_sofia <- c(feat2, 'flag_class')
fit <- sofia(flag_class ~ ., data=tot[,feat_sofia], lambda = 1e-3, iiterations = 1e+25, random_seed = 13560,
learner_type = 'logreg-pegasos', #c("pegasos", "sgd-svm","passive-aggressive", "margin-perceptron", "romma", "logreg-pegasos"),
eta_type = 'pegasos', #c("pegasos", "basic", "constant"),
loop_type = 'balanced-stochastic', #c("stochastic","balanced-stochastic", "rank", "roc", "query-norm-rank","combined-ranking", "combined-roc"),
rank_step_probability = 0.5,
passive_aggressive_c = 1e+07, passive_aggressive_lambda = 1e+1, dimensionality = 60,
perceptron_margin_size = 1, training_objective = F, hash_mask_bits = 0,
verbose = T, reserve = 1
)
p <- predict(fit, newdata=tot[,feat_sofia], prediction_type = "logistic")
all$sofia_bal_meta_raw <- as.vector(p)
fit <- sofia(flag_class ~ ., data=tot[,feat_sofia], lambda = 1e-3, iiterations = 1e+25, random_seed = 13560,
learner_type = 'logreg-pegasos', #c("pegasos", "sgd-svm","passive-aggressive", "margin-perceptron", "romma", "logreg-pegasos"),
eta_type = 'pegasos', #c("pegasos", "basic", "constant"),
loop_type = 'combined-roc', #c("stochastic","balanced-stochastic", "rank", "roc", "query-norm-rank","combined-ranking", "combined-roc"),
rank_step_probability = 0.5,
passive_aggressive_c = 1e+07, passive_aggressive_lambda = 1e+1, dimensionality = 60,
perceptron_margin_size = 1, training_objective = F, hash_mask_bits = 0,
verbose = T, reserve = 1
)
p <- predict(fit, newdata=tot[feat_sofia], prediction_type = "logistic")
all$sofia_roc_meta_raw <- as.vector(p)
head(all)
save(all, file='data/9_train_validation_test_20151108_meta_sofia_v3.RData')
save(all, file='data/9_train_validation_test_20151108_meta_sofia_v3.RData')
