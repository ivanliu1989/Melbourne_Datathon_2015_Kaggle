}
if(platform == "mac"){
c <- odbcDriverConnect(connection=paste0("server=",host,";database=",db,";uid=",user,";pwd=",pass,
";Port=1433;driver=FreeTDS;TDS_Version=7.0;"))
if(class(c) == 'RODBC'){
writeLines("Successfilly opened connection to db")
return(c)
}else{
writeLines(paste0("Error opening connection: ", as.character(c)))
}
}
}
# rdp.csgplatform.com:5685
cn <- connect(host='localhost', db='comtrade_source', user='sa', pass='Servian1', platform="mac")
ch1 <- odbcConnect(dsn="sqlserver01", uid="sa", pwd="Servian1")
ch1 <- odbcConnect(dsn="sqlserver01", uid="sa", pwd="Servian1")
#load ODBC library
library(RODBC)
odbcDataSources()
ch1 <- odbcConnect(dsn="sqlserver01", uid="sa", pwd="Servian1")
# #load data
# data <- sqlFetch(cn, 'myTable', colnames=FALSE, rows_at_time=1000)
# #load data
# data <- sqlQuery(cn, "select * from myTable")
# status <- sqlGetResults(cn, as.is = FALSE, errors = TRUE, max = 0, buffsize = 1000000,
#                         nullstring = NA_character_, na.strings = "NA", believeNRows = TRUE, dec = getOption("dec"),
#                         stringsAsFactors = default.stringsAsFactors())
# #read with odbcQuery
# status  <- odbcQuery(cn, "select * from myTable")
# data <- odbcFetchRows(cn, max = 0, buffsize = 10000, nullstring = NA_character_, believeNRows = TRUE)
# error <- odbcGetErrMsg(cn)
### function ###
connect <- function(host, db, user=NULL, pass=NULL, platform="win" ){
# TODO: Check input paramaters and add a branch for SQL auth on windows
if(platform == "win"){
c <- odbcDriverConnect(connection=paste0("server=",host,";database=",db,
";trusted_connection=true;Port=1433;driver={SQL Server};TDS_Version=7.0;"))
if(class(c) == 'RODBC'){
writeLines("Successfilly opened connection to db")
return(c)
}else{
writeLines(paste0("Error opening connection: ", as.character(c)))
}
}
if(platform == "mac"){
c <- odbcDriverConnect(connection=paste0("server=",host,";database=",db,";uid=",user,";pwd=",pass,
";Port=1433;driver=FreeTDS;TDS_Version=7.0;"))
if(class(c) == 'RODBC'){
writeLines("Successfilly opened connection to db")
return(c)
}else{
writeLines(paste0("Error opening connection: ", as.character(c)))
}
}
}
# rdp.csgplatform.com:5685
cn <- connect(host='localhost', db='comtrade_source', user='sa', pass='Servian1', platform="mac")
ch1 <- odbcConnect(dsn="sqlserver01", uid="sa", pwd="Servian1")
require(Rserve)
Rserve()
Rserve()
install.packages(c("boot", "gtools", "manipulate"))
head(train)
rm(list = ls()); gc()
require(data.table);require(caret);require(doMC);require(ROCR)
registerDoMC(core=3)
load('data/new/cv_data_log_extend.RData')
install.packages("manipulate")
data("iris")
# this is a little tweak so that things line up nicely later on
iris$Species <- factor(iris$Species,
levels = c("versicolor","virginica","setosa"))
head(iris)
ound(cor(iris[,1:4]), 2)
round(cor(iris[,1:4]), 2)
pc <- princomp(iris[,1:4], cor=TRUE, scores=TRUE)
pc
summary(pc)
plot(pc,type="lines")
biplot(pc)
library(rgl)
plot3d(pc$scores[,1:3], col=iris$Species)
plot3d(pc$scores[,1:3], col=iris$Species)
plot3d(pc$scores[,1:3])#, col=iris$Species)
text3d(pc$scores[,1:3],texts=rownames(iris))
text3d(pc$loadings[,1:3], texts=rownames(pc$loadings), col="red")
coords <- NULL
for (i in 1:nrow(pc$loadings)) {
coords <- rbind(coords, rbind(c(0,0,0),pc$loadings[i,1:3]))
}
for (i in 1:nrow(pc$loadings)) {
coords <- rbind(coords, rbind(c(0,0,0),pc$loadings[i,1:3]))
}
lines3d(coords, col="red", lwd=4)
set.seed(42)
cl <- kmeans(iris[,1:4],3)
iris$cluster <- as.factor(cl$cluster)
plot3d(pc$scores[,1:3], col=iris$cluster, main="k-means clusters")
plot3d(pc$scores[,1:3], col=iris$Species, main="actual species")
with(iris, table(cluster, Species))
data("iris")
# this is a little tweak so that things line up nicely later on
iris$Species <- factor(iris$Species,
levels = c("versicolor","virginica","setosa"))
head(iris)
round(cor(iris[,1:4]), 2)
pc <- princomp(iris[,1:4], cor=TRUE, scores=TRUE)
summary(pc)
plot(pc,type="lines")
biplot(pc)
library(rgl)
plot3d(pc$scores[,1:3], col=iris$Species)
text3d(pc$scores[,1:3],texts=rownames(iris))
text3d(pc$loadings[,1:3], texts=rownames(pc$loadings), col="red")
coords <- NULL
for (i in 1:nrow(pc$loadings)) {
coords <- rbind(coords, rbind(c(0,0,0),pc$loadings[i,1:3]))
}
lines3d(coords, col="red", lwd=4)
set.seed(42)
cl <- kmeans(iris[,1:4],3)
iris$cluster <- as.factor(cl$cluster)
plot3d(pc$scores[,1:3], col=iris$cluster, main="k-means clusters")
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
p <- read.csv('libffm/output_file_valid_train.csv', header = F);val <- validation; val$Y <- p[,1]
head(p)
p <- read.csv('libffm/output_file_valid_train.csv', header = T);
head(p)
p <- read.csv('libffm/output_file_valid_train.csv', header = T, sep = " ");
head(p)
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(xgboost);library(pROC);require(randomForest);library(Rtsne);require(data.table);library(caret);library(RSofia);library(h2o)
# load('data/Ivan_Train_Test_Scale_Center_20151116.RData');ls()
load('data/9_train_validation_test_20151122.RData');ls()
options(scipen=999);set.seed(19890624)
#
# write.csv(test, '../python_test_ffm_meta.csv', row.names = F)
# write.csv(train, '../python_train_ffm_meta.csv', row.names = F)
# write.csv(validation, '../python_validation_ffm_meta.csv', row.names = F)
# write.csv(total, '../python_total_ffm_meta.csv', row.names = F)
# write.csv(p_gbm, 'ReadyForBlending/xgboost_1.csv', row.names = F)
p <- read.csv('libffm/output_file_valid_train.csv', header = T, sep = " ");val <- validation; val$Y <- p[,2]
tot_invest <- aggregate(INVEST ~ ACCOUNT_ID,data=val, sum, na.rm=T); names(tot_invest) <- c('ACCOUNT_ID', 'TOT_INVEST')
val <- merge(val, tot_invest, all.x = TRUE, all.y = FALSE, by = c('ACCOUNT_ID'))
val$INVEST_PERCENT <- val$INVEST/val$TOT_INVEST * val$Y
pred_fin <- aggregate(INVEST_PERCENT ~ ACCOUNT_ID, data=val, sum, na.rm=F)
pred_fin2 <- aggregate(Y ~ ACCOUNT_ID, data=val, mean, na.rm=F)
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, 0)
### Model Performance ###
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin[,2]);print(auc(rocobj)) # Invest * Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin2[,2]);print(auc(rocobj)) # Average Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
prediction <- as.factor(ifelse(pred_fin[,2] >=0.5, 1, 0))
confusionMatrix(as.factor(val_fin$PRED_PROFIT_LOSS_3), prediction)
p <- read.csv('libffm/output_file_valid_train.csv', header = T, sep = " ");val <- validation; val$Y <- p[,2]
tot_invest <- aggregate(INVEST ~ ACCOUNT_ID,data=val, sum, na.rm=T); names(tot_invest) <- c('ACCOUNT_ID', 'TOT_INVEST')
val <- merge(val, tot_invest, all.x = TRUE, all.y = FALSE, by = c('ACCOUNT_ID'))
val$INVEST_PERCENT <- val$INVEST/val$TOT_INVEST * val$Y
pred_fin <- aggregate(INVEST_PERCENT ~ ACCOUNT_ID, data=val, sum, na.rm=F)
pred_fin2 <- aggregate(Y ~ ACCOUNT_ID, data=val, mean, na.rm=F)
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, 0)
### Model Performance ###
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin[,2]);print(auc(rocobj)) # Invest * Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin2[,2]);print(auc(rocobj)) # Average Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
prediction <- as.factor(ifelse(pred_fin[,2] >=0.5, 1, 0))
confusionMatrix(as.factor(val_fin$PRED_PROFIT_LOSS_3), prediction)
head(p)
p <- read.csv('libffm/output_file_valid_train.csv', header = T, sep = " ");val <- validation; val$Y <- p[,3]
tot_invest <- aggregate(INVEST ~ ACCOUNT_ID,data=val, sum, na.rm=T); names(tot_invest) <- c('ACCOUNT_ID', 'TOT_INVEST')
val <- merge(val, tot_invest, all.x = TRUE, all.y = FALSE, by = c('ACCOUNT_ID'))
val$INVEST_PERCENT <- val$INVEST/val$TOT_INVEST * val$Y
pred_fin <- aggregate(INVEST_PERCENT ~ ACCOUNT_ID, data=val, sum, na.rm=F)
pred_fin2 <- aggregate(Y ~ ACCOUNT_ID, data=val, mean, na.rm=F)
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, 0)
### Model Performance ###
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin[,2]);print(auc(rocobj)) # Invest * Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin2[,2]);print(auc(rocobj)) # Average Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
prediction <- as.factor(ifelse(pred_fin[,2] >=0.5, 1, 0))
confusionMatrix(as.factor(val_fin$PRED_PROFIT_LOSS_3), prediction)
p <- read.csv('libffm/output_file.csv', header = T, sep = " ");val <- validation; val$Y <- p[,3]
tot_invest <- aggregate(INVEST ~ ACCOUNT_ID,data=val, sum, na.rm=T); names(tot_invest) <- c('ACCOUNT_ID', 'TOT_INVEST')
val <- merge(val, tot_invest, all.x = TRUE, all.y = FALSE, by = c('ACCOUNT_ID'))
val$INVEST_PERCENT <- val$INVEST/val$TOT_INVEST * val$Y
pred_fin <- aggregate(INVEST_PERCENT ~ ACCOUNT_ID, data=val, sum, na.rm=F)
pred_fin2 <- aggregate(Y ~ ACCOUNT_ID, data=val, mean, na.rm=F)
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, 0)
### Model Performance ###
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin[,2]);print(auc(rocobj)) # Invest * Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin2[,2]);print(auc(rocobj)) # Average Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
prediction <- as.factor(ifelse(pred_fin[,2] >=0.5, 1, 0))
confusionMatrix(as.factor(val_fin$PRED_PROFIT_LOSS_3), prediction)
### New Calc
rocobj <- roc(val$flag_class, val$Y);print(auc(rocobj))
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(xgboost);library(pROC);require(randomForest);library(Rtsne);require(data.table);library(caret);library(RSofia);library(h2o)
load('data/9_train_validation_test_20151122.RData');ls()
options(scipen=999);set.seed(19890624)
library(h2o)
localH2O <- h2o.init(ip = 'localhost', port = 54321, max_mem_size = '12g')
test <- test#train[train$EVENT_ID %in% c(101183757,101183885,101184013),]#validation
train <- total#train[!train$EVENT_ID %in% c(101183757,101183885,101184013),]
train$flag_class <- ifelse(train$flag_class == 'Y', 1, 0)
test$flag_class <- ifelse(test$flag_class == 'Y', 1, 0)
validation$flag_class <- ifelse(validation$flag_class == 'Y', 1, 0)
train$flag_class <- as.factor(train$flag_class)
test$flag_class <- as.factor(test$flag_class)
validation$flag_class <- as.factor(validation$flag_class)
train_df <- as.h2o(localH2O, train) # train
test_df <- as.h2o(localH2O, test) # test
valid_df <- as.h2o(localH2O, validation) # test
independent <- colnames(train)[c(3:(ncol(train)-2))]
dependent <- "flag_class"
fit <-
h2o.glm(
y = dependent, x = independent, training_frame = train_df, #train_df | total_df
max_iterations = 100, beta_epsilon = 1e-4, solver = "L_BFGS", #IRLSM  L_BFGS
standardize = T, family = 'binomial', link = 'logit', alpha = 0.5, # 1 lasso 0 ridge
lambda = 0, lambda_search = T, nlambda = 55, #lambda_min_ratio = 1e-08,
intercept = T
#higher_accuracy = T, disable_line_search = F, use_all_factor_levels = T,strong_rules = T
)
p <- as.data.frame(h2o.predict(object = fit, newdata = valid_df))
val <- validation
val$Y <- p[,3]
tot_invest <- aggregate(INVEST ~ ACCOUNT_ID,data=val, sum, na.rm=T); names(tot_invest) <- c('ACCOUNT_ID', 'TOT_INVEST')
val <- merge(val, tot_invest, all.x = TRUE, all.y = FALSE, by = c('ACCOUNT_ID'))
val$INVEST_PERCENT <- val$INVEST/val$TOT_INVEST * val$Y
pred_fin <- aggregate(INVEST_PERCENT ~ ACCOUNT_ID, data=val, sum, na.rm=F)
pred_fin2 <- aggregate(Y ~ ACCOUNT_ID, data=val, mean, na.rm=F)
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, 0)
### Model Performance ###
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin[,2]);print(auc(rocobj)) # Invest * Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin2[,2]);print(auc(rocobj)) # Average Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
prediction <- as.factor(ifelse(pred_fin[,2] >=0.5, 1, 0))
confusionMatrix(as.factor(val_fin$PRED_PROFIT_LOSS_3), prediction)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
write.csv(p, paste0('ReadyForBlending/submission/h2o_glm/submission_h2o_glm_20151128_.csv'))
fit <-
h2o.randomForest(
y = dependent, x = independent, training_frame = train_df, mtries = -1,
ntrees = 800, max_depth = 16, sample.rate = 0.632, min_rows = 1,
nbins = 20, nbins_cats = 1024, binomial_double_trees = T
)
fit
p <- as.data.frame(h2o.predict(object = fit, newdata = valid_df))
val <- validation
val$Y <- p[,3]
tot_invest <- aggregate(INVEST ~ ACCOUNT_ID,data=val, sum, na.rm=T); names(tot_invest) <- c('ACCOUNT_ID', 'TOT_INVEST')
val <- merge(val, tot_invest, all.x = TRUE, all.y = FALSE, by = c('ACCOUNT_ID'))
val$INVEST_PERCENT <- val$INVEST/val$TOT_INVEST * val$Y
pred_fin <- aggregate(INVEST_PERCENT ~ ACCOUNT_ID, data=val, sum, na.rm=F)
pred_fin2 <- aggregate(Y ~ ACCOUNT_ID, data=val, mean, na.rm=F)
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, 0)
### Model Performance ###
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin[,2]);print(auc(rocobj)) # Invest * Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin2[,2]);print(auc(rocobj)) # Average Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
prediction <- as.factor(ifelse(pred_fin[,2] >=0.5, 1, 0))
confusionMatrix(as.factor(val_fin$PRED_PROFIT_LOSS_3), prediction)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
write.csv(p, paste0('ReadyForBlending/submission/h2o_rf/submission_h2o_rf_20151128_.csv'))
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(xgboost);library(pROC);require(randomForest);library(Rtsne);require(data.table);library(caret);library(RSofia);library(h2o)
load('data/9_train_validation_test_20151122.RData');ls()
options(scipen=999);set.seed(19890624)
library(h2o)
localH2O <- h2o.init(ip = 'localhost', port = 54321, max_mem_size = '12g')
test <- test#train[train$EVENT_ID %in% c(101183757,101183885,101184013),]#validation
train <- train#train[!train$EVENT_ID %in% c(101183757,101183885,101184013),]
train$flag_class <- ifelse(train$flag_class == 'Y', 1, 0)
test$flag_class <- ifelse(test$flag_class == 'Y', 1, 0)
validation$flag_class <- ifelse(validation$flag_class == 'Y', 1, 0)
train$flag_class <- as.factor(train$flag_class)
test$flag_class <- as.factor(test$flag_class)
validation$flag_class <- as.factor(validation$flag_class)
train_df <- as.h2o(localH2O, train) # train
test_df <- as.h2o(localH2O, test) # test
valid_df <- as.h2o(localH2O, validation) # test
independent <- colnames(train)[c(3:(ncol(train)-2))]
dependent <- "flag_class"
# gbm
# fit <- h2o.gbm(
#     y = dependent, x = independent, training_frame = train_df, #train_df | total_df
#     ntrees = 1000, max_depth = 8, min_rows = 2,
#     learn_rate = 0.15, distribution = "bernoulli", nbins_cats = 20,  #AUTO
#     importance = F
# )
# # p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
# p <- as.data.frame(h2o.predict(object = fit, newdata = valid_df))
# deeplearning
fit <-
h2o.deeplearning(
y = dependent, x = independent, training_frame = train_df, overwrite_with_best_model = T, #autoencoder
use_all_factor_levels = T, activation = "RectifierWithDropout",#TanhWithDropout "RectifierWithDropout"
hidden = c(300,150,75), epochs = 12, train_samples_per_iteration = -2, adaptive_rate = T, rho = 0.99,
epsilon = 1e-6, rate = 0.02, rate_decay = 0.9, momentum_start = 0.9, momentum_stable = 0.99,
nesterov_accelerated_gradient = T, input_dropout_ratio = 0.25, hidden_dropout_ratios = c(0.15,0.15,0.15),
l1 = NULL, l2 = NULL, loss = 'CrossEntropy', classification_stop = 0.01,
diagnostics = T, variable_importances = F, fast_mode = F, ignore_const_cols = T,
force_load_balance = T, replicate_training_data = T, shuffle_training_data = T
)
p <- as.data.frame(h2o.predict(object = fit, newdata = valid_df))
val <- validation
val$Y <- p[,3]
tot_invest <- aggregate(INVEST ~ ACCOUNT_ID,data=val, sum, na.rm=T); names(tot_invest) <- c('ACCOUNT_ID', 'TOT_INVEST')
val <- merge(val, tot_invest, all.x = TRUE, all.y = FALSE, by = c('ACCOUNT_ID'))
val$INVEST_PERCENT <- val$INVEST/val$TOT_INVEST * val$Y
pred_fin <- aggregate(INVEST_PERCENT ~ ACCOUNT_ID, data=val, sum, na.rm=F)
pred_fin2 <- aggregate(Y ~ ACCOUNT_ID, data=val, mean, na.rm=F)
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, 0)
### Model Performance ###
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin[,2]);print(auc(rocobj)) # Invest * Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin2[,2]);print(auc(rocobj)) # Average Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
prediction <- as.factor(ifelse(pred_fin[,2] >=0.5, 1, 0))
confusionMatrix(as.factor(val_fin$PRED_PROFIT_LOSS_3), prediction)
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(xgboost);library(pROC);require(randomForest);library(Rtsne);require(data.table);library(caret);library(RSofia);library(h2o)
load('data/9_train_validation_test_20151122.RData');ls()
options(scipen=999);set.seed(19890624)
library(h2o)
localH2O <- h2o.init(ip = 'localhost', port = 54321, max_mem_size = '12g')
test <- test#train[train$EVENT_ID %in% c(101183757,101183885,101184013),]#validation
train <- total#train[!train$EVENT_ID %in% c(101183757,101183885,101184013),]
train$flag_class <- ifelse(train$flag_class == 'Y', 1, 0)
test$flag_class <- ifelse(test$flag_class == 'Y', 1, 0)
validation$flag_class <- ifelse(validation$flag_class == 'Y', 1, 0)
train$flag_class <- as.factor(train$flag_class)
test$flag_class <- as.factor(test$flag_class)
validation$flag_class <- as.factor(validation$flag_class)
train_df <- as.h2o(localH2O, train) # train
test_df <- as.h2o(localH2O, test) # test
valid_df <- as.h2o(localH2O, validation) # test
independent <- colnames(train)[c(3:(ncol(train)-2))]
dependent <- "flag_class"
# gbm
# fit <- h2o.gbm(
#     y = dependent, x = independent, training_frame = train_df, #train_df | total_df
#     ntrees = 1000, max_depth = 8, min_rows = 2,
#     learn_rate = 0.15, distribution = "bernoulli", nbins_cats = 20,  #AUTO
#     importance = F
# )
# # p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
# p <- as.data.frame(h2o.predict(object = fit, newdata = valid_df))
# deeplearning
for(i in 1:50){
set.seed(i*8)
fit <-
h2o.deeplearning(
y = dependent, x = independent, training_frame = train_df, overwrite_with_best_model = T, #autoencoder
use_all_factor_levels = T, activation = "RectifierWithDropout",#TanhWithDropout "RectifierWithDropout"
hidden = c(300,150,75), epochs = 12, train_samples_per_iteration = -2, adaptive_rate = T, rho = 0.99,
epsilon = 1e-6, rate = 0.02, rate_decay = 0.9, momentum_start = 0.9, momentum_stable = 0.99,
nesterov_accelerated_gradient = T, input_dropout_ratio = 0.25, hidden_dropout_ratios = c(0.15,0.15,0.15),
l1 = NULL, l2 = NULL, loss = 'CrossEntropy', classification_stop = 0.01,
diagnostics = T, variable_importances = F, fast_mode = F, ignore_const_cols = T,
force_load_balance = T, replicate_training_data = T, shuffle_training_data = T
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
# p <- as.data.frame(h2o.predict(object = fit, newdata = valid_df))
write.csv(p, paste0('ReadyForBlending/submission/h2o_nnet/submission_h2o_nnet_20151128_',i,'.csv'))
}
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(xgboost);library(pROC);require(randomForest);library(Rtsne);require(data.table);library(caret);library(RSofia);library(h2o)
load('data/9_train_validation_test_20151122.RData');ls()
options(scipen=999);set.seed(19890624)
library(h2o)
localH2O <- h2o.init(ip = 'localhost', port = 54321, max_mem_size = '12g')
test <- test#train[train$EVENT_ID %in% c(101183757,101183885,101184013),]#validation
train <- train#train[!train$EVENT_ID %in% c(101183757,101183885,101184013),]
train$flag_class <- ifelse(train$flag_class == 'Y', 1, 0)
test$flag_class <- ifelse(test$flag_class == 'Y', 1, 0)
validation$flag_class <- ifelse(validation$flag_class == 'Y', 1, 0)
train$flag_class <- as.factor(train$flag_class)
test$flag_class <- as.factor(test$flag_class)
validation$flag_class <- as.factor(validation$flag_class)
train_df <- as.h2o(localH2O, train) # train
test_df <- as.h2o(localH2O, test) # test
valid_df <- as.h2o(localH2O, validation) # test
independent <- colnames(train)[c(3:(ncol(train)-2))]
dependent <- "flag_class"
fit <-
h2o.deeplearning(
y = dependent, x = independent, training_frame = train_df, overwrite_with_best_model = T, #autoencoder
use_all_factor_levels = T, activation = "RectifierWithDropout",#TanhWithDropout "RectifierWithDropout"
hidden = c(300,150,75), epochs = 12, train_samples_per_iteration = -2, adaptive_rate = T, rho = 0.99,
epsilon = 1e-6, rate = 0.02, rate_decay = 0.9, momentum_start = 0.9, momentum_stable = 0.99,
nesterov_accelerated_gradient = T, input_dropout_ratio = 0.15, hidden_dropout_ratios = c(0.25,0.25,0.25),
l1 = NULL, l2 = NULL, loss = 'CrossEntropy', classification_stop = 0.01,
diagnostics = T, variable_importances = F, fast_mode = F, ignore_const_cols = T,
force_load_balance = T, replicate_training_data = T, shuffle_training_data = T
)
p <- as.data.frame(h2o.predict(object = fit, newdata = valid_df))
val <- validation
val$Y <- p[,3]
tot_invest <- aggregate(INVEST ~ ACCOUNT_ID,data=val, sum, na.rm=T); names(tot_invest) <- c('ACCOUNT_ID', 'TOT_INVEST')
val <- merge(val, tot_invest, all.x = TRUE, all.y = FALSE, by = c('ACCOUNT_ID'))
val$INVEST_PERCENT <- val$INVEST/val$TOT_INVEST * val$Y
pred_fin <- aggregate(INVEST_PERCENT ~ ACCOUNT_ID, data=val, sum, na.rm=F)
pred_fin2 <- aggregate(Y ~ ACCOUNT_ID, data=val, mean, na.rm=F)
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, 0)
### Model Performance ###
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin[,2]);print(auc(rocobj)) # Invest * Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin2[,2]);print(auc(rocobj)) # Average Possibility
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
prediction <- as.factor(ifelse(pred_fin[,2] >=0.5, 1, 0))
confusionMatrix(as.factor(val_fin$PRED_PROFIT_LOSS_3), prediction)
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(xgboost);library(pROC);require(randomForest);library(Rtsne);require(data.table);library(caret);library(RSofia);library(h2o)
load('data/9_train_validation_test_20151122.RData');ls()
options(scipen=999);set.seed(19890624)
library(h2o)
localH2O <- h2o.init(ip = 'localhost', port = 54321, max_mem_size = '12g')
test <- test_n#train[train$EVENT_ID %in% c(101183757,101183885,101184013),]#validation
train <- train_n#train[!train$EVENT_ID %in% c(101183757,101183885,101184013),]
train$flag_class <- ifelse(train$flag_class == 'Y', 1, 0)
test$flag_class <- ifelse(test$flag_class == 'Y', 1, 0)
validation$flag_class <- ifelse(validation$flag_class == 'Y', 1, 0)
train$flag_class <- as.factor(train$flag_class)
test$flag_class <- as.factor(test$flag_class)
validation$flag_class <- as.factor(validation$flag_class)
train_df <- as.h2o(localH2O, train) # train
test_df <- as.h2o(localH2O, test) # test
valid_df <- as.h2o(localH2O, validation) # test
independent <- colnames(train)[c(3:(ncol(train)-2))]
dependent <- "flag_class"
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(xgboost);library(pROC);require(randomForest);library(Rtsne);require(data.table);library(caret);library(RSofia);library(h2o)
load('data/9_train_validation_test_20151122.RData');ls()
options(scipen=999);set.seed(19890624)
library(h2o)
localH2O <- h2o.init(ip = 'localhost', port = 54321, max_mem_size = '12g')
test <- test_n#train[train$EVENT_ID %in% c(101183757,101183885,101184013),]#validation
train <- total_n#train[!train$EVENT_ID %in% c(101183757,101183885,101184013),]
train$flag_class <- ifelse(train$flag_class == 'Y', 1, 0)
test$flag_class <- ifelse(test$flag_class == 'Y', 1, 0)
validation$flag_class <- ifelse(validation$flag_class == 'Y', 1, 0)
train$flag_class <- as.factor(train$flag_class)
test$flag_class <- as.factor(test$flag_class)
validation$flag_class <- as.factor(validation$flag_class)
train_df <- as.h2o(localH2O, train) # train
test_df <- as.h2o(localH2O, test) # test
valid_df <- as.h2o(localH2O, validation) # test
independent <- colnames(train)[c(3:(ncol(train)-2))]
dependent <- "flag_class"
independent
fit <-
h2o.glm(
y = dependent, x = independent, training_frame = train_df, #train_df | total_df
max_iterations = 100, beta_epsilon = 1e-4, solver = "L_BFGS", #IRLSM  L_BFGS
standardize = T, family = 'binomial', link = 'logit', alpha = 0.5, # 1 lasso 0 ridge
lambda = 0, lambda_search = T, nlambda = 55, #lambda_min_ratio = 1e-08,
intercept = T
#higher_accuracy = T, disable_line_search = F, use_all_factor_levels = T,strong_rules = T
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
write.csv(p, paste0('ReadyForBlending/submission/test_n/h2o_glm/submission_h2o_glm_n_20151128_.csv'))
# random forest
fit <-
h2o.randomForest(
y = dependent, x = independent, training_frame = train_df, mtries = -1,
ntrees = 800, max_depth = 16, sample.rate = 0.632, min_rows = 1,
nbins = 20, nbins_cats = 1024, binomial_double_trees = T
)
p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
# p <- as.data.frame(h2o.predict(object = fit, newdata = valid_df))
write.csv(p, paste0('ReadyForBlending/submission/test_n/h2o_rf/submission_h2o_rf_n_20151128_.csv'))
for(i in 1:50){
set.seed(i*8)
fit <-
h2o.deeplearning(
y = dependent, x = independent, training_frame = train_df, overwrite_with_best_model = T, #autoencoder
use_all_factor_levels = T, activation = "RectifierWithDropout",#TanhWithDropout "RectifierWithDropout"
hidden = c(300,150,75), epochs = 12, train_samples_per_iteration = -2, adaptive_rate = T, rho = 0.99,
epsilon = 1e-6, rate = 0.02, rate_decay = 0.9, momentum_start = 0.9, momentum_stable = 0.99,
nesterov_accelerated_gradient = T, input_dropout_ratio = 0.25, hidden_dropout_ratios = c(0.25,0.25,0.25),
l1 = NULL, l2 = NULL, loss = 'CrossEntropy', classification_stop = 0.01,
diagnostics = T, variable_importances = F, fast_mode = F, ignore_const_cols = T,
force_load_balance = T, replicate_training_data = T, shuffle_training_data = T
)
# p <- as.data.frame(h2o.predict(object = fit, newdata = test_df))
p <- as.data.frame(h2o.predict(object = fit, newdata = valid_df))
write.csv(p, paste0('ReadyForBlending/submission/test_n/h2o_nnet/submission_h2o_nnet_n_20151129_',i,'.csv'))
}
