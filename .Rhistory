# status  <- odbcQuery(cn, "select * from myTable")
# data <- odbcFetchRows(cn, max = 0, buffsize = 10000, nullstring = NA_character_, believeNRows = TRUE)
# error <- odbcGetErrMsg(cn)
### function ###
connect <- function(host, db, user=NULL, pass=NULL, platform="win" ){
# TODO: Check input paramaters and add a branch for SQL auth on windows
if(platform == "win"){
c <- odbcDriverConnect(connection=paste0("server=",host,";database=",db,
";trusted_connection=true;Port=1433;driver={SQL Server};TDS_Version=7.0;"))
if(class(c) == 'RODBC'){
writeLines("Successfilly opened connection to db")
return(c)
}else{
writeLines(paste0("Error opening connection: ", as.character(c)))
}
}
if(platform == "mac"){
c <- odbcDriverConnect(connection=paste0("server=",host,";database=",db,";uid=",user,";pwd=",pass,
";Port=1433;driver=FreeTDS;TDS_Version=7.0;"))
if(class(c) == 'RODBC'){
writeLines("Successfilly opened connection to db")
return(c)
}else{
writeLines(paste0("Error opening connection: ", as.character(c)))
}
}
}
# rdp.csgplatform.com:5685
cn <- connect(host='localhost', db='comtrade_source', user='sa', pass='Servian1', platform="mac")
#load ODBC library
library(RODBC)
odbcDataSources()
# #load data
# data <- sqlFetch(cn, 'myTable', colnames=FALSE, rows_at_time=1000)
# #load data
# data <- sqlQuery(cn, "select * from myTable")
# status <- sqlGetResults(cn, as.is = FALSE, errors = TRUE, max = 0, buffsize = 1000000,
#                         nullstring = NA_character_, na.strings = "NA", believeNRows = TRUE, dec = getOption("dec"),
#                         stringsAsFactors = default.stringsAsFactors())
# #read with odbcQuery
# status  <- odbcQuery(cn, "select * from myTable")
# data <- odbcFetchRows(cn, max = 0, buffsize = 10000, nullstring = NA_character_, believeNRows = TRUE)
# error <- odbcGetErrMsg(cn)
### function ###
connect <- function(host, db, user=NULL, pass=NULL, platform="win" ){
# TODO: Check input paramaters and add a branch for SQL auth on windows
if(platform == "win"){
c <- odbcDriverConnect(connection=paste0("server=",host,";database=",db,
";trusted_connection=true;Port=1433;driver={SQL Server};TDS_Version=7.0;"))
if(class(c) == 'RODBC'){
writeLines("Successfilly opened connection to db")
return(c)
}else{
writeLines(paste0("Error opening connection: ", as.character(c)))
}
}
if(platform == "mac"){
c <- odbcDriverConnect(connection=paste0("server=",host,";database=",db,";uid=",user,";pwd=",pass,
";Port=1433;driver=FreeTDS;TDS_Version=7.0;"))
if(class(c) == 'RODBC'){
writeLines("Successfilly opened connection to db")
return(c)
}else{
writeLines(paste0("Error opening connection: ", as.character(c)))
}
}
}
# rdp.csgplatform.com:5685
cn <- connect(host='localhost', db='comtrade_source', user='sa', pass='Servian1', platform="mac")
ch1 <- odbcConnect(dsn="sqlserver01", uid="sa", pwd="Servian1")
ch1 <- odbcConnect(dsn="sqlserver01", uid="sa", pwd="Servian1")
#load ODBC library
library(RODBC)
odbcDataSources()
ch1 <- odbcConnect(dsn="sqlserver01", uid="sa", pwd="Servian1")
# #load data
# data <- sqlFetch(cn, 'myTable', colnames=FALSE, rows_at_time=1000)
# #load data
# data <- sqlQuery(cn, "select * from myTable")
# status <- sqlGetResults(cn, as.is = FALSE, errors = TRUE, max = 0, buffsize = 1000000,
#                         nullstring = NA_character_, na.strings = "NA", believeNRows = TRUE, dec = getOption("dec"),
#                         stringsAsFactors = default.stringsAsFactors())
# #read with odbcQuery
# status  <- odbcQuery(cn, "select * from myTable")
# data <- odbcFetchRows(cn, max = 0, buffsize = 10000, nullstring = NA_character_, believeNRows = TRUE)
# error <- odbcGetErrMsg(cn)
### function ###
connect <- function(host, db, user=NULL, pass=NULL, platform="win" ){
# TODO: Check input paramaters and add a branch for SQL auth on windows
if(platform == "win"){
c <- odbcDriverConnect(connection=paste0("server=",host,";database=",db,
";trusted_connection=true;Port=1433;driver={SQL Server};TDS_Version=7.0;"))
if(class(c) == 'RODBC'){
writeLines("Successfilly opened connection to db")
return(c)
}else{
writeLines(paste0("Error opening connection: ", as.character(c)))
}
}
if(platform == "mac"){
c <- odbcDriverConnect(connection=paste0("server=",host,";database=",db,";uid=",user,";pwd=",pass,
";Port=1433;driver=FreeTDS;TDS_Version=7.0;"))
if(class(c) == 'RODBC'){
writeLines("Successfilly opened connection to db")
return(c)
}else{
writeLines(paste0("Error opening connection: ", as.character(c)))
}
}
}
# rdp.csgplatform.com:5685
cn <- connect(host='localhost', db='comtrade_source', user='sa', pass='Servian1', platform="mac")
ch1 <- odbcConnect(dsn="sqlserver01", uid="sa", pwd="Servian1")
require(Rserve)
Rserve()
Rserve()
install.packages(c("boot", "gtools", "manipulate"))
head(train)
rm(list = ls()); gc()
require(data.table);require(caret);require(doMC);require(ROCR)
registerDoMC(core=3)
load('data/new/cv_data_log_extend.RData')
install.packages("manipulate")
data("iris")
# this is a little tweak so that things line up nicely later on
iris$Species <- factor(iris$Species,
levels = c("versicolor","virginica","setosa"))
head(iris)
ound(cor(iris[,1:4]), 2)
round(cor(iris[,1:4]), 2)
pc <- princomp(iris[,1:4], cor=TRUE, scores=TRUE)
pc
summary(pc)
plot(pc,type="lines")
biplot(pc)
library(rgl)
plot3d(pc$scores[,1:3], col=iris$Species)
plot3d(pc$scores[,1:3], col=iris$Species)
plot3d(pc$scores[,1:3])#, col=iris$Species)
text3d(pc$scores[,1:3],texts=rownames(iris))
text3d(pc$loadings[,1:3], texts=rownames(pc$loadings), col="red")
coords <- NULL
for (i in 1:nrow(pc$loadings)) {
coords <- rbind(coords, rbind(c(0,0,0),pc$loadings[i,1:3]))
}
for (i in 1:nrow(pc$loadings)) {
coords <- rbind(coords, rbind(c(0,0,0),pc$loadings[i,1:3]))
}
lines3d(coords, col="red", lwd=4)
set.seed(42)
cl <- kmeans(iris[,1:4],3)
iris$cluster <- as.factor(cl$cluster)
plot3d(pc$scores[,1:3], col=iris$cluster, main="k-means clusters")
plot3d(pc$scores[,1:3], col=iris$Species, main="actual species")
with(iris, table(cluster, Species))
data("iris")
# this is a little tweak so that things line up nicely later on
iris$Species <- factor(iris$Species,
levels = c("versicolor","virginica","setosa"))
head(iris)
round(cor(iris[,1:4]), 2)
pc <- princomp(iris[,1:4], cor=TRUE, scores=TRUE)
summary(pc)
plot(pc,type="lines")
biplot(pc)
library(rgl)
plot3d(pc$scores[,1:3], col=iris$Species)
text3d(pc$scores[,1:3],texts=rownames(iris))
text3d(pc$loadings[,1:3], texts=rownames(pc$loadings), col="red")
coords <- NULL
for (i in 1:nrow(pc$loadings)) {
coords <- rbind(coords, rbind(c(0,0,0),pc$loadings[i,1:3]))
}
lines3d(coords, col="red", lwd=4)
set.seed(42)
cl <- kmeans(iris[,1:4],3)
iris$cluster <- as.factor(cl$cluster)
plot3d(pc$scores[,1:3], col=iris$cluster, main="k-means clusters")
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
setwd('C:\\Users\\iliu2\\Documents\\datathon\\Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(caret);library(pROC);library(doMC)
registerDoMC(cores = 4)
load('data/9_train_validation_test_TREE_2.RData');ls()
# load('data/9_train_validation_test_ONEHOT_1.RData');ls()
####################
# Config 1 #########
####################
train$flag_class <- as.factor(train$flag_class)
fitControl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 2,
classProbs = TRUE,
summaryFunction = twoClassSummary)
Grid <-  expand.grid(mtry=8)
# Grid <-  expand.grid(nrounds = 100, max_depth = 8, eta = 0.05) # xgbTree
# Grid <-  expand.grid(sigma = 1, C = 0.01) # svmRadial
# Grid <-  expand.grid(size = 80, decay = 0.1) # nnet
# Grid <-  expand.grid(fL=0.01, usekernel=F) # nb
# Grid <-  expand.grid(nIter=20) # LogitBoost
# Grid <-  expand.grid(n.trees = 180, interaction.depth = 6, shrinkage = 0.01) # gbm
Training
set.seed(825)
fit <- train(flag_class ~ ., data=train[,-c(1,2,57,59)], # classification
method = "rf",
trControl = fitControl,
tuneGrid = Grid,
# preProcess = c('center', 'scale'),
metric ='ROC',
verbose = T)
fit
fitImp <- varImp(fit, scale = T)
as.data.frame(fitImp[1])
write.csv(as.data.frame(fitImp[1]),'model/variable_imp_rf_new_feat.csv',quote = FALSE,row.names = T)
val <- validation#[!validation$COUNTRY_OF_RESIDENCE_NAME %in% c('Qatar'),]
p <- predict(fit, newdata=val, type = 'prob')
val$Y <- p$Y
val$PRED_PROFIT_LOSS <- (val$Y - 0.5) * val$INVEST * 2
pred_fin <- aggregate(PRED_PROFIT_LOSS ~ ACCOUNT_ID, data=val, sum, na.rm=F)
pred_fin$PRED_PROFIT_LOSS_2 <- ifelse(pred_fin$PRED_PROFIT_LOSS > 0, 1, ifelse(pred_fin$PRED_PROFIT_LOSS < 0, 0, 0.5))
pred_fin2 <- aggregate(Y ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
# p_rf <- p
# p_lg <- p
# p$Y <- (p_rf$Y + p_lg$Y)/2
#########################
### Model Performance ###
#########################
v <- merge(val_fin,pred_fin,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
v <- merge(v,pred_fin2,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
# With a roc object:
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$PRED_PROFIT_LOSS_2);auc(rocobj) # Invest * Possibility
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$Y);auc(rocobj) # Average Possibility
# rocobj <- roc(val$flag_class, p$Y);auc(rocobj) # Single Events
# Partial AUC:
auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE)
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
setwd('C:\\Users\\iliu2\\Documents\\datathon\\Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(h2o);library(pROC);library(doMC)
# load('data/9_train_validation_test_TREE_2.RData');ls()
load('data/9_train_validation_test_ONEHOT_1.RData');ls()
set.seed(18)
registerDoMC(cores = 4)
localH2O <- h2o.init(ip = 'localhost', port = 54321, max_mem_size = '12g')
train$flag_class <- as.factor(train$flag_class); levels(train$flag_class) <- c(0,1)
val_class <- validation$flag_class
validation$flag_class <- as.factor(validation$flag_class); levels(validation$flag_class) <- c(0,1)
train_df <- as.h2o(localH2O, train)
validation_df <- as.h2o(localH2O, validation)
colnames(train_df[,3:(ncol(train_df)-3)])
colnames(train_df)
independent <- colnames(train_df[,3:(ncol(train_df)-3)])
dependent <- "flag_class"
#     fit <- h2o.gbm(y = dependent, x = independent, training_frame = train_df,
#                    ntrees = 200, max_depth = 8, min_rows = 1,
#                    learn_rate = 0.25, distribution= "bernoulli", nbins = 20,  #AUTO
#                    nbins_cats = 1024, seed = 8)
#     fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
#                             classification_stop = -1, activation="TanhWithDropout",#TanhWithDropout "RectifierWithDropout"
#                             hidden=c(512,256), hidden_dropout_ratios = c(0.15,0.15), input_dropout_ratio = 0.5,
#                             epochs=5, adaptive_rate = T, rho = 0.99, epsilon = 1e-10, # 1e-4
#                             rate_decay=0.8,rate=0.1,momentum_start = 0.5, momentum_stable=0.99,
#                             nesterov_accelerated_gradient = T, loss='CrossEntropy', l2=3e-6, max_w2=2,
#                             seed=8,variable_importances=F,sparse= F,diagnostics=T,shuffle_training_data=T)# classification=T, autoencoder = F,
#     fit <- h2o.randomForest(y = dependent, x = independent, training_frame = train_df, #validation_frame
#                             ntree=100, max_depth=10, mtries=8, sample_rate=0.8,
#                             min_rows=1, nbins_cats=1024, nbins = 10, seed=8, binomial_double_trees = T)
# fit <- h2o.naiveBayes(y = dependent, x = independent, data = train_df, laplace = 0)
fit <- h2o.glm(y = dependent, x = independent, data = train_df,
family='binomial', link='logit',alpha = 0.5, # 1 lasso 0 ridge
lambda = 1e-08, lambda_search = T, nlambda = 8, lambda.min.ratio = 0.1,
strong_rules = T, standardize = T, intercept = T, use_all_factor_levels = F,
epsilon = 1e-4, iter.max = 100, higher_accuracy = T, disable_line_search = F)
##################
### Prediction ###
##################
val <- validation
pred <- h2o.predict(object = fit, newdata = validation_df)
val <- cbind(val, as.data.frame(pred[,3]))
val$PRED_PROFIT_LOSS <- (val[,ncol(val)] - 0.5) * val$INVEST * 2
pred_fin <- aggregate(PRED_PROFIT_LOSS ~ ACCOUNT_ID, data=val, sum, na.rm=T)
pred_fin$PRED_PROFIT_LOSS_2 <- ifelse(pred_fin$PRED_PROFIT_LOSS > 0, 1, ifelse(pred_fin$PRED_PROFIT_LOSS < 0, 0, 0.5))
pred_fin2 <- aggregate(X1 ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
#########################
### Model Performance ###
#########################
v <- merge(val_fin,pred_fin,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
v <- merge(v,pred_fin2,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
# print(fit)
# With a roc object:
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$PRED_PROFIT_LOSS_2)
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v[,6])
# Partial AUC:
# print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
# Plot plot(rocobj)
# Performance Selection
perf_new <- auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE)
rocobj;perf_new
head(pred)
pref_lg <- pred
fit <- h2o.naiveBayes(y = dependent, x = independent, data = train_df, laplace = 0)
#             fit <- h2o.glm(y = dependent, x = independent, data = train_df,
#                            family='binomial', link='logit',alpha = 0.5, # 1 lasso 0 ridge
#                            lambda = 1e-08, lambda_search = T, nlambda = 8, lambda.min.ratio = 0.1,
#                            strong_rules = T, standardize = T, intercept = T, use_all_factor_levels = F,
#                            epsilon = 1e-4, iter.max = 100, higher_accuracy = T, disable_line_search = F)
##################
### Prediction ###
##################
val <- validation
pred <- h2o.predict(object = fit, newdata = validation_df)
val <- cbind(val, as.data.frame(pred[,3]))
val$PRED_PROFIT_LOSS <- (val[,ncol(val)] - 0.5) * val$INVEST * 2
pred_fin <- aggregate(PRED_PROFIT_LOSS ~ ACCOUNT_ID, data=val, sum, na.rm=T)
pred_fin$PRED_PROFIT_LOSS_2 <- ifelse(pred_fin$PRED_PROFIT_LOSS > 0, 1, ifelse(pred_fin$PRED_PROFIT_LOSS < 0, 0, 0.5))
pred_fin2 <- aggregate(X1 ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
#########################
### Model Performance ###
#########################
v <- merge(val_fin,pred_fin,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
v <- merge(v,pred_fin2,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
# print(fit)
# With a roc object:
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$PRED_PROFIT_LOSS_2)
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v[,6])
# Partial AUC:
# print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
# Plot plot(rocobj)
# Performance Selection
perf_new <- auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE)
rocobj;perf_new
head(pred)
load('data/9_train_validation_test_TREE_1.RData');ls()
# load('data/9_train_validation_test_ONEHOT_1.RData');ls()
################
### Register ###
################
set.seed(18)
registerDoMC(cores = 4)
localH2O <- h2o.init(ip = 'localhost', port = 54321, max_mem_size = '12g')
train$flag_class <- as.factor(train$flag_class); levels(train$flag_class) <- c(0,1)
val_class <- validation$flag_class
validation$flag_class <- as.factor(validation$flag_class); levels(validation$flag_class) <- c(0,1)
# total$flag_class <- as.factor(total$flag_class); levels(total$flag_class) <- c(0,1)
######################
### Feature Select ###
######################
# train <- train[,-c(27:30, 44:46)] # skew, kurt, bl_ratio
# total_df <- as.h2o(localH2O, total)
train_df <- as.h2o(localH2O, train)
validation_df <- as.h2o(localH2O, validation)
# test_df <- as.h2o(localH2O, test)
independent <- colnames(train_df[,3:(ncol(train_df)-3)])
dependent <- "flag_class"
fit <- h2o.glm(y = dependent, x = independent, data = train_df,
family='binomial', link='logit',alpha = 0.5, # 1 lasso 0 ridge
lambda = 1e-08, lambda_search = T, nlambda = 8, lambda.min.ratio = 0.1,
strong_rules = T, standardize = T, intercept = T, use_all_factor_levels = F,
epsilon = 1e-4, iter.max = 100, higher_accuracy = T, disable_line_search = F)
##################
### Prediction ###
##################
val <- validation
pred <- h2o.predict(object = fit, newdata = validation_df)
val <- cbind(val, as.data.frame(pred[,3]))
val$PRED_PROFIT_LOSS <- (val[,ncol(val)] - 0.5) * val$INVEST * 2
pred_fin <- aggregate(PRED_PROFIT_LOSS ~ ACCOUNT_ID, data=val, sum, na.rm=T)
pred_fin$PRED_PROFIT_LOSS_2 <- ifelse(pred_fin$PRED_PROFIT_LOSS > 0, 1, ifelse(pred_fin$PRED_PROFIT_LOSS < 0, 0, 0.5))
pred_fin2 <- aggregate(X1 ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
#########################
### Model Performance ###
#########################
v <- merge(val_fin,pred_fin,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
v <- merge(v,pred_fin2,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
# print(fit)
# With a roc object:
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$PRED_PROFIT_LOSS_2)
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v[,6])
# Partial AUC:
# print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
# Plot plot(rocobj)
# Performance Selection
perf_new <- auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE)
rocobj;perf_new
h2o.shutdown(localH2O)
pref_lg
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
setwd('C:\\Users\\iliu2\\Documents\\datathon\\Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(h2o);library(pROC);library(doMC)
load('data/9_train_validation_test_TREE_1.RData');ls()
# load('data/9_train_validation_test_ONEHOT_1.RData');ls()
################
### Register ###
################
set.seed(18)
registerDoMC(cores = 4)
localH2O <- h2o.init(ip = 'localhost', port = 54321, max_mem_size = '12g')
train$flag_class <- as.factor(train$flag_class); levels(train$flag_class) <- c(0,1)
val_class <- validation$flag_class
validation$flag_class <- as.factor(validation$flag_class); levels(validation$flag_class) <- c(0,1)
# total$flag_class <- as.factor(total$flag_class); levels(total$flag_class) <- c(0,1)
######################
### Feature Select ###
######################
# train <- train[,-c(27:30, 44:46)] # skew, kurt, bl_ratio
# total_df <- as.h2o(localH2O, total)
train_df <- as.h2o(localH2O, train)
validation_df <- as.h2o(localH2O, validation)
# test_df <- as.h2o(localH2O, test)
independent <- colnames(train_df[,3:(ncol(train_df)-3)])
dependent <- "flag_class"
#     fit <- h2o.gbm(y = dependent, x = independent, training_frame = train_df,
#                    ntrees = 200, max_depth = 8, min_rows = 1,
#                    learn_rate = 0.25, distribution= "bernoulli", nbins = 20,  #AUTO
#                    nbins_cats = 1024, seed = 8)
#     fit <- h2o.deeplearning(y = dependent, x = independent, data = train_df,
#                             classification_stop = -1, activation="TanhWithDropout",#TanhWithDropout "RectifierWithDropout"
#                             hidden=c(512,256), hidden_dropout_ratios = c(0.15,0.15), input_dropout_ratio = 0.5,
#                             epochs=5, adaptive_rate = T, rho = 0.99, epsilon = 1e-10, # 1e-4
#                             rate_decay=0.8,rate=0.1,momentum_start = 0.5, momentum_stable=0.99,
#                             nesterov_accelerated_gradient = T, loss='CrossEntropy', l2=3e-6, max_w2=2,
#                             seed=8,variable_importances=F,sparse= F,diagnostics=T,shuffle_training_data=T)# classification=T, autoencoder = F,
#     fit <- h2o.randomForest(y = dependent, x = independent, training_frame = train_df, #validation_frame
#                             ntree=100, max_depth=10, mtries=8, sample_rate=0.8,
#                             min_rows=1, nbins_cats=1024, nbins = 10, seed=8, binomial_double_trees = T)
# fit <- h2o.naiveBayes(y = dependent, x = independent, data = train_df, laplace = 0)
fit <- h2o.glm(y = dependent, x = independent, data = train_df,
family='binomial', link='logit',alpha = 0.5, # 1 lasso 0 ridge
lambda = 1e-08, lambda_search = T, nlambda = 8, lambda.min.ratio = 0.1,
strong_rules = T, standardize = T, intercept = T, use_all_factor_levels = F,
epsilon = 1e-4, iter.max = 100, higher_accuracy = T, disable_line_search = F)
##################
### Prediction ###
##################
val <- validation
pred <- h2o.predict(object = fit, newdata = validation_df)
val <- cbind(val, as.data.frame(pred[,3]))
val$PRED_PROFIT_LOSS <- (val[,ncol(val)] - 0.5) * val$INVEST * 2
pred_fin <- aggregate(PRED_PROFIT_LOSS ~ ACCOUNT_ID, data=val, sum, na.rm=T)
pred_fin$PRED_PROFIT_LOSS_2 <- ifelse(pred_fin$PRED_PROFIT_LOSS > 0, 1, ifelse(pred_fin$PRED_PROFIT_LOSS < 0, 0, 0.5))
pred_fin2 <- aggregate(X1 ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
#########################
### Model Performance ###
#########################
v <- merge(val_fin,pred_fin,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
v <- merge(v,pred_fin2,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
# print(fit)
# With a roc object:
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$PRED_PROFIT_LOSS_2)
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v[,6])
# Partial AUC:
# print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
# Plot plot(rocobj)
# Performance Selection
perf_new <- auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE)
rocobj;perf_new
pref_lg <- pred
fit
pref_lg <- pred
fit <- h2o.randomForest(y = dependent, x = independent, training_frame = train_df, #validation_frame
ntree=100, max_depth=10, mtries=8, sample_rate=0.8,
min_rows=1, nbins_cats=1024, nbins = 10, seed=8, binomial_double_trees = T)
# fit <- h2o.naiveBayes(y = dependent, x = independent, data = train_df, laplace = 0)
#             fit <- h2o.glm(y = dependent, x = independent, data = train_df,
#                            family='binomial', link='logit',alpha = 0.5, # 1 lasso 0 ridge
#                            lambda = 1e-08, lambda_search = T, nlambda = 8, lambda.min.ratio = 0.1,
#                            strong_rules = T, standardize = T, intercept = T, use_all_factor_levels = F,
#                            epsilon = 1e-4, iter.max = 100, higher_accuracy = T, disable_line_search = F)
##################
### Prediction ###
##################
val <- validation
pred <- h2o.predict(object = fit, newdata = validation_df)
val <- cbind(val, as.data.frame(pred[,3]))
val$PRED_PROFIT_LOSS <- (val[,ncol(val)] - 0.5) * val$INVEST * 2
pred_fin <- aggregate(PRED_PROFIT_LOSS ~ ACCOUNT_ID, data=val, sum, na.rm=T)
pred_fin$PRED_PROFIT_LOSS_2 <- ifelse(pred_fin$PRED_PROFIT_LOSS > 0, 1, ifelse(pred_fin$PRED_PROFIT_LOSS < 0, 0, 0.5))
pred_fin2 <- aggregate(X1 ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
#########################
### Model Performance ###
#########################
v <- merge(val_fin,pred_fin,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
v <- merge(v,pred_fin2,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
# print(fit)
# With a roc object:
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$PRED_PROFIT_LOSS_2)
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v[,6])
# Partial AUC:
# print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
# Plot plot(rocobj)
# Performance Selection
perf_new <- auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE)
rocobj;perf_new
