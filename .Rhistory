#                             classification_stop = -1, activation="TanhWithDropout",#TanhWithDropout "RectifierWithDropout"
#                             hidden=c(512,256), hidden_dropout_ratios = c(0.15,0.15), input_dropout_ratio = 0.5,
#                             epochs=5, adaptive_rate = T, rho = 0.99, epsilon = 1e-10, # 1e-4
#                             rate_decay=0.8,rate=0.1,momentum_start = 0.5, momentum_stable=0.99,
#                             nesterov_accelerated_gradient = T, loss='CrossEntropy', l2=3e-6, max_w2=2,
#                             seed=8,variable_importances=F,sparse= F,diagnostics=T,shuffle_training_data=T)# classification=T, autoencoder = F,
#     fit <- h2o.randomForest(y = dependent, x = independent, training_frame = train_df, #validation_frame
#                             ntree=100, max_depth=10, mtries=8, sample_rate=0.8,
#                             min_rows=1, nbins_cats=1024, nbins = 10, seed=8, binomial_double_trees = T)
fit <- h2o.naiveBayes(y = dependent, x = independent, training_frame = train_df, laplace = 0)
#     fit <- h2o.glm(y = dependent, x = independent, data = train_df,
#                    family='binomial', link='logit',alpha = 0.5, # 1 lasso 0 ridge
#                    lambda = 1e-5, lambda_search = T, nlambda = 10, lambda.min.ratio = 0.4,
#                    strong_rules = T, standardize = T, intercept = T, use_all_factor_levels = F,
#                    epsilon = 1e-4, iter.max = 100, higher_accuracy = T, disable_line_search = F)
##################
### Prediction ###
##################
val <- validation
pred <- h2o.predict(object = fit, newdata = validation_df)
val <- cbind(val, as.data.frame(pred[,3]))
val$PRED_PROFIT_LOSS <- (val[,ncol(val)] - 0.5) * val$INVEST * 2
pred_fin <- aggregate(PRED_PROFIT_LOSS ~ ACCOUNT_ID, data=val, sum, na.rm=F)
pred_fin$PRED_PROFIT_LOSS_2 <- ifelse(pred_fin$PRED_PROFIT_LOSS > 0, 1, ifelse(pred_fin$PRED_PROFIT_LOSS < 0, 0, 0.5))
pred_fin2 <- aggregate(X1 ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
#########################
### Model Performance ###
#########################
# print(fit)
# With a roc object:
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin$PRED_PROFIT_LOSS_2); print(auc(rocobj))
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin2[,2]); print(auc(rocobj))
# Partial AUC:
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
# Plot plot(rocobj)
?"h2o.naiveBayes"
fit <- h2o.naiveBayes(y = dependent, x = independent, data = train_df, laplace = 0)
#     fit <- h2o.glm(y = dependent, x = independent, data = train_df,
#                    family='binomial', link='logit',alpha = 0.5, # 1 lasso 0 ridge
#                    lambda = 1e-5, lambda_search = T, nlambda = 10, lambda.min.ratio = 0.4,
#                    strong_rules = T, standardize = T, intercept = T, use_all_factor_levels = F,
#                    epsilon = 1e-4, iter.max = 100, higher_accuracy = T, disable_line_search = F)
##################
### Prediction ###
##################
val <- validation
pred <- h2o.predict(object = fit, newdata = validation_df)
val <- cbind(val, as.data.frame(pred[,3]))
val$PRED_PROFIT_LOSS <- (val[,ncol(val)] - 0.5) * val$INVEST * 2
pred_fin <- aggregate(PRED_PROFIT_LOSS ~ ACCOUNT_ID, data=val, sum, na.rm=F)
pred_fin$PRED_PROFIT_LOSS_2 <- ifelse(pred_fin$PRED_PROFIT_LOSS > 0, 1, ifelse(pred_fin$PRED_PROFIT_LOSS < 0, 0, 0.5))
pred_fin2 <- aggregate(X1 ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
#########################
### Model Performance ###
#########################
# print(fit)
# With a roc object:
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin$PRED_PROFIT_LOSS_2); print(auc(rocobj))
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin2[,2]); print(auc(rocobj))
# Partial AUC:
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
# Plot plot(rocobj)
val <- validation
pred <- h2o.predict(object = fit, newdata = validation_df)
val <- cbind(val, as.data.frame(pred[,3]))
val$PRED_PROFIT_LOSS <- (val[,ncol(val)] - 0.5) * val$INVEST * 2
pred_fin <- aggregate(PRED_PROFIT_LOSS ~ ACCOUNT_ID, data=val, sum, na.rm=F)
pred_fin$PRED_PROFIT_LOSS_2 <- ifelse(pred_fin$PRED_PROFIT_LOSS > 0, 1, ifelse(pred_fin$PRED_PROFIT_LOSS < 0, 0, 0.5))
pred_fin2 <- aggregate(X1 ~ ACCOUNT_ID, data=val, mean, na.rm=F)
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin$PRED_PROFIT_LOSS_2); print(auc(rocobj))
dim(val_fin)
dim(pred_fin)
pred_fin <- aggregate(PRED_PROFIT_LOSS ~ ACCOUNT_ID, data=val, sum, na.rm=T)
dim(pred_fin)
pred <- h2o.predict(object = fit, newdata = validation_df)
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
setwd('C:\\Users\\iliu2\\Documents\\datathon\\Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(h2o);library(pROC);library(doMC)
load('data/6_train_validation_test_center_scale_no_dummy.RData');ls()
# load('data/6_train_validation_test_center_scale_no_dummy_2.RData');ls()
################
### Register ###
################
registerDoMC(cores = 4)
localH2O <- h2o.init(ip = 'localhost', port = 54321, max_mem_size = '12g')
train$flag_class <- as.factor(train$flag_class); levels(train$flag_class) <- c(0,1)
val_class <- validation$flag_class
validation$flag_class <- as.factor(validation$flag_class); levels(validation$flag_class) <- c(0,1)
# total$flag_class <- as.factor(total$flag_class); levels(total$flag_class) <- c(0,1)
######################
### Feature Select ###
######################
# train <- train[,-c(27:30, 44:46)] # skew, kurt, bl_ratio
# total_df <- as.h2o(localH2O, total)
train_df <- as.h2o(localH2O, train)
validation_df <- as.h2o(localH2O, validation)
# test_df <- as.h2o(localH2O, test)
independent <- colnames(train_df[,3:(ncol(train_df)-3)])
dependent <- "flag_class"
fit <- h2o.naiveBayes(y = dependent, x = independent, data = train_df, laplace = 0)
val <- validation
pred <- h2o.predict(object = fit, newdata = validation_df)
val <- cbind(val, as.data.frame(pred[,3]))
val$PRED_PROFIT_LOSS <- (val[,ncol(val)] - 0.5) * val$INVEST * 2
pred_fin <- aggregate(PRED_PROFIT_LOSS ~ ACCOUNT_ID, data=val, sum, na.rm=T)
pred_fin$PRED_PROFIT_LOSS_2 <- ifelse(pred_fin$PRED_PROFIT_LOSS > 0, 1, ifelse(pred_fin$PRED_PROFIT_LOSS < 0, 0, 0.5))
pred_fin2 <- aggregate(X1 ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
#########################
### Model Performance ###
#########################
# print(fit)
# With a roc object:
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin$PRED_PROFIT_LOSS_2); print(auc(rocobj))
head(val_fin)
v <- merge(val_fin,pred_fin,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$PRED_PROFIT_LOSS_2); print(auc(rocobj))
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
head(pred)
fit <- h2o.naiveBayes(y = dependent, x = independent, data = train_df, laplace = 10)
val <- validation
pred <- h2o.predict(object = fit, newdata = validation_df)
head(pred)
fit <- h2o.naiveBayes(y = dependent, x = independent, data = train_df, laplace = 0)
val <- validation
pred <- h2o.predict(object = fit, newdata = validation_df)
val <- cbind(val, as.data.frame(pred[,3]))
val$PRED_PROFIT_LOSS <- (val[,ncol(val)] - 0.5) * val$INVEST * 2
pred_fin <- aggregate(PRED_PROFIT_LOSS ~ ACCOUNT_ID, data=val, sum, na.rm=T)
pred_fin$PRED_PROFIT_LOSS_2 <- ifelse(pred_fin$PRED_PROFIT_LOSS > 0, 1, ifelse(pred_fin$PRED_PROFIT_LOSS < 0, 0, 0.5))
pred_fin2 <- aggregate(X1 ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
#########################
### Model Performance ###
#########################
v <- merge(val_fin,pred_fin,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
# print(fit)
# With a roc object:
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$PRED_PROFIT_LOSS_2); print(auc(rocobj))
rocobj <- roc(val_fin$PRED_PROFIT_LOSS_3, pred_fin2[,2]); print(auc(rocobj))
# Partial AUC:
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
# Plot plot(rocobj)
v <- merge(v,pred_fin2,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
head(merge(val_fin,pred_fin,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID'))
head(pred_fin2)
val <- validation
pred <- h2o.predict(object = fit, newdata = validation_df)
val <- cbind(val, as.data.frame(pred[,3]))
val$PRED_PROFIT_LOSS <- (val[,ncol(val)] - 0.5) * val$INVEST * 2
pred_fin <- aggregate(PRED_PROFIT_LOSS ~ ACCOUNT_ID, data=val, sum, na.rm=T)
pred_fin$PRED_PROFIT_LOSS_2 <- ifelse(pred_fin$PRED_PROFIT_LOSS > 0, 1, ifelse(pred_fin$PRED_PROFIT_LOSS < 0, 0, 0.5))
pred_fin2 <- aggregate(X1 ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
#########################
### Model Performance ###
#########################
v <- merge(val_fin,pred_fin,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
v <- merge(v,pred_fin2,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
# print(fit)
head(v)
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v[,6]); print(auc(rocobj))
print(auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE))
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
setwd('C:\\Users\\iliu2\\Documents\\datathon\\Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(caret);library(pROC);library(doMC)
registerDoMC(cores = 4)
load('data/6_train_validation_test_center_scale_no_dummy.RData');ls()
# load('data/6_train_validation_test_center_scale_no_dummy_2.RData');ls()
dim(train)
head(train)
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
setwd('C:\\Users\\iliu2\\Documents\\datathon\\Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(caret);library(pROC);library(doMC)
registerDoMC(cores = 4)
load('data/6_train_validation_test_center_scale_no_dummy.RData');ls()
# load('data/6_train_validation_test_center_scale_no_dummy_2.RData');ls()
######################
### Training Model ###
######################
# Config
train$flag_class <- as.factor(train$flag_class)
fitControl <- trainControl(method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary)
# Grid <-  expand.grid(mtry=7)
# Grid <-  expand.grid(nrounds = 100, max_depth = 8, eta = 0.05) # xgbTree
# Grid <-  expand.grid(sigma = 1, C = 0.1) # svmRaidal
# Grid <-  expand.grid(size = 80, decay = 0.1) # nnet
# Grid <-  expand.grid(fL=0.01, usekernel=F) # nb
Grid <-  expand.grid(nIter=20) # LogitBoost
# Grid <-  expand.grid(n.trees = 180, interaction.depth = 6, shrinkage = 0.01) # gbm
# Training
set.seed(825)
fit <- train(flag_class ~ ., data=train[,-c(1,2,47,49)], # classification
method = "rf",
trControl = fitControl,
tuneGrid = Grid,
# preProcess = c('center', 'scale'),
metric ='ROC',
verbose = T)
Grid <-  expand.grid(nIter=20) # LogitBoost
# Grid <-  expand.grid(n.trees = 180, interaction.depth = 6, shrinkage = 0.01) # gbm
# Training
set.seed(825)
fit <- train(flag_class ~ ., data=train[,-c(1,2,47,49)], # classification
method = "LogitBoost",
trControl = fitControl,
tuneGrid = Grid,
# preProcess = c('center', 'scale'),
metric ='ROC',
verbose = T)
fit
fitImp <- varImp(fit, scale = T)
ls
val <- validation#[!validation$COUNTRY_OF_RESIDENCE_NAME %in% c('Qatar'),]
p <- predict(fit, newdata=val, type = 'prob')
val$Y <- p$Y
val$PRED_PROFIT_LOSS <- (val$Y - 0.5) * val$INVEST * 2
pred_fin <- aggregate(PRED_PROFIT_LOSS ~ ACCOUNT_ID, data=val, sum, na.rm=F)
pred_fin$PRED_PROFIT_LOSS_2 <- ifelse(pred_fin$PRED_PROFIT_LOSS > 0, 1, ifelse(pred_fin$PRED_PROFIT_LOSS < 0, 0, 0.5))
pred_fin2 <- aggregate(Y ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
# p_rf <- p
# p_lg <- p
# p$Y <- (p_rf$Y + p_lg$Y)/2
#########################
### Model Performance ###
#########################
v <- merge(val_fin,pred_fin,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
v <- merge(v,pred_fin2,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$PRED_PROFIT_LOSS_2);auc(rocobj)
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$Y);auc(rocobj) # Average Possibility
rocobj <- roc(val$flag_class, p$Y);auc(rocobj) # Single Events
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$Y);auc(rocobj) # Average Possibility
auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE)
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$PRED_PROFIT_LOSS_2);auc(rocobj) # Invest * Possibility
auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE)
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
setwd('C:\\Users\\iliu2\\Documents\\datathon\\Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(caret);library(pROC);library(doMC)
registerDoMC(cores = 4)
load('data/6_train_validation_test_center_scale_no_dummy.RData');ls()
# load('data/6_train_validation_test_center_scale_no_dummy_2.RData');ls()
####################
# Config 1 #########
####################
train$flag_class <- as.factor(train$flag_class)
fitControl <- trainControl(method = "none",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary)
# Grid <-  expand.grid(mtry=7)
# Grid <-  expand.grid(nrounds = 100, max_depth = 8, eta = 0.05) # xgbTree
# Grid <-  expand.grid(sigma = 1, C = 0.1) # svmRaidal
# Grid <-  expand.grid(size = 80, decay = 0.1) # nnet
# Grid <-  expand.grid(fL=0.01, usekernel=F) # nb
Grid <-  expand.grid(nIter=20) # LogitBoost
# Grid <-  expand.grid(n.trees = 180, interaction.depth = 6, shrinkage = 0.01) # gbm
# Training
set.seed(825)
fit <- train(flag_class ~ ., data=train[,-c(1,2,47,49)], # classification
method = "LogitBoost",
trControl = fitControl,
tuneGrid = Grid,
# preProcess = c('center', 'scale'),
metric ='ROC',
verbose = T)
### Predict
val <- validation#[!validation$COUNTRY_OF_RESIDENCE_NAME %in% c('Qatar'),]
p <- predict(fit, newdata=val, type = 'prob')
val$Y <- p$Y
val$PRED_PROFIT_LOSS <- (val$Y - 0.5) * val$INVEST * 2
pred_fin <- aggregate(PRED_PROFIT_LOSS ~ ACCOUNT_ID, data=val, sum, na.rm=F)
pred_fin$PRED_PROFIT_LOSS_2 <- ifelse(pred_fin$PRED_PROFIT_LOSS > 0, 1, ifelse(pred_fin$PRED_PROFIT_LOSS < 0, 0, 0.5))
pred_fin2 <- aggregate(Y ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
# p_rf <- p
# p_lg <- p
# p$Y <- (p_rf$Y + p_lg$Y)/2
#########################
### Model Performance ###
#########################
v <- merge(val_fin,pred_fin,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
v <- merge(v,pred_fin2,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
# With a roc object:
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$PRED_PROFIT_LOSS_2);auc(rocobj) # Invest * Possibility
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$Y);auc(rocobj) # Average Possibility
auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE)
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$PRED_PROFIT_LOSS_2);auc(rocobj) # Invest * Possibility
head(test)
head(train)
names(train)
fitControl <- trainControl(method = "none",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary)
# Grid <-  expand.grid(mtry=7)
# Grid <-  expand.grid(nrounds = 100, max_depth = 8, eta = 0.05) # xgbTree
# Grid <-  expand.grid(sigma = 1, C = 0.1) # svmRaidal
# Grid <-  expand.grid(size = 80, decay = 0.1) # nnet
# Grid <-  expand.grid(fL=0.01, usekernel=F) # nb
Grid <-  expand.grid(nIter=20) # LogitBoost
# Grid <-  expand.grid(n.trees = 180, interaction.depth = 6, shrinkage = 0.01) # gbm
# Training
set.seed(825)
fit <- train(flag_class ~ ., data=train[,-c(1,2,47,49, 31,32)], # classification
method = "LogitBoost",
trControl = fitControl,
tuneGrid = Grid,
# preProcess = c('center', 'scale'),
metric ='ROC',
verbose = T)
####################
# Config 2 #########
####################
# fitControl2 <- trainControl(method = "adaptive_cv",
#                             number = 5,
#                             repeats = 2,
#                             ## Estimate class probabilities
#                             classProbs = TRUE,
#                             ## Evaluate performance using
#                             ## the following function
#                             summaryFunction = twoClassSummary,
#                             ## Adaptive resampling information:
#                             adaptive = list(min = 8,
#                                             alpha = 0.05,
#                                             method = "BT", #gls for Linear
#                                             complete = TRUE))
#
# set.seed(825)
# svmFit2 <- train(flag_class ~ ., data=train[,-c(1,2,53)],
#                  method = "rf", #svmRadial
#                  trControl = fitControl2,
#                  # preProc = c("center", "scale"),
#                  tuneLength = 8,
#                  metric = "ROC")
####################
# Config 3 #########
####################
# fitControl3 <- trainControl(method = "repeatedcv",
#                             number = 5,
#                             repeats = 2,
#                             classProbs = TRUE,
#                             summaryFunction = twoClassSummary,
#                             search = "random")
#
# set.seed(825)
# fit3 <- train(flag_class ~ ., data=train[,-c(1,2,47,49)],
#                  method = "rf", #svmRadial
#                  trControl = fitControl3,
#                  # preProc = c("center", "scale"),
#                  tuneLength = 30,
#                  metric = "ROC")
#
# # Plot
# ggplot(fit3) +
#     geom_smooth(se = FALSE, span = .8, method = loess) +
#     theme(legend.position = "top")
# Variable Imp
fitImp <- varImp(fit, scale = T)
##################
### Validation ###
##################
### Predict
val <- validation#[!validation$COUNTRY_OF_RESIDENCE_NAME %in% c('Qatar'),]
p <- predict(fit, newdata=val, type = 'prob')
val$Y <- p$Y
val$PRED_PROFIT_LOSS <- (val$Y - 0.5) * val$INVEST * 2
pred_fin <- aggregate(PRED_PROFIT_LOSS ~ ACCOUNT_ID, data=val, sum, na.rm=F)
pred_fin$PRED_PROFIT_LOSS_2 <- ifelse(pred_fin$PRED_PROFIT_LOSS > 0, 1, ifelse(pred_fin$PRED_PROFIT_LOSS < 0, 0, 0.5))
pred_fin2 <- aggregate(Y ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
# p_rf <- p
# p_lg <- p
# p$Y <- (p_rf$Y + p_lg$Y)/2
#########################
### Model Performance ###
#########################
v <- merge(val_fin,pred_fin,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
v <- merge(v,pred_fin2,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
# With a roc object:
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$PRED_PROFIT_LOSS_2);auc(rocobj) # Invest * Possibility
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$Y);auc(rocobj) # Average Possibility
# rocobj <- roc(val$flag_class, p$Y);auc(rocobj) # Single Events
# Partial AUC:
auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE)
# rf 0.5933 | 0.5779 (country, BL, SKE, KURT) | 0.5796 (BL, SKE, KURT)
# lg 0.6529
p_lg <- p
fitControl <- trainControl(method = "none",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary)
Grid <-  expand.grid(mtry=7)
# Grid <-  expand.grid(nrounds = 100, max_depth = 8, eta = 0.05) # xgbTree
# Grid <-  expand.grid(sigma = 1, C = 0.1) # svmRaidal
# Grid <-  expand.grid(size = 80, decay = 0.1) # nnet
# Grid <-  expand.grid(fL=0.01, usekernel=F) # nb
# Grid <-  expand.grid(nIter=20) # LogitBoost
# Grid <-  expand.grid(n.trees = 180, interaction.depth = 6, shrinkage = 0.01) # gbm
# Training
set.seed(825)
fit <- train(flag_class ~ ., data=train[,-c(1,2,47,49)], # classification
method = "rf",
trControl = fitControl,
tuneGrid = Grid,
# preProcess = c('center', 'scale'),
metric ='ROC',
verbose = T)
fit
val <- validation#[!validation$COUNTRY_OF_RESIDENCE_NAME %in% c('Qatar'),]
p <- predict(fit, newdata=val, type = 'prob')
val$Y <- p$Y
val$PRED_PROFIT_LOSS <- (val$Y - 0.5) * val$INVEST * 2
pred_fin <- aggregate(PRED_PROFIT_LOSS ~ ACCOUNT_ID, data=val, sum, na.rm=F)
pred_fin$PRED_PROFIT_LOSS_2 <- ifelse(pred_fin$PRED_PROFIT_LOSS > 0, 1, ifelse(pred_fin$PRED_PROFIT_LOSS < 0, 0, 0.5))
pred_fin2 <- aggregate(Y ~ ACCOUNT_ID, data=val, mean, na.rm=F)
### Validation
val_fin <- aggregate(flag_regr ~ ACCOUNT_ID, data=val, sum, na.rm=F)
val_fin$PRED_PROFIT_LOSS_3 <- ifelse(val_fin$flag_regr > 0, 1, ifelse(val_fin$flag_regr < 0, 0, 0.5))
# p_rf <- p
# p_lg <- p
# p$Y <- (p_rf$Y + p_lg$Y)/2
#########################
### Model Performance ###
#########################
v <- merge(val_fin,pred_fin,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
v <- merge(v,pred_fin2,all.x = TRUE,all.y = FALSE, by = 'ACCOUNT_ID')
# With a roc object:
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$PRED_PROFIT_LOSS_2);auc(rocobj) # Invest * Possibility
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$Y);auc(rocobj) # Average Possibility
# rocobj <- roc(val$flag_class, p$Y);auc(rocobj) # Single Events
auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE)
rocobj <- roc(v$PRED_PROFIT_LOSS_3, v$PRED_PROFIT_LOSS_2);auc(rocobj) # Invest * Possibility
auc(rocobj, partial.auc=c(1, .8), partial.auc.focus="se", partial.auc.correct=TRUE)
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
setwd('C:\\Users\\iliu2\\Documents\\datathon\\Melbourne_Datathon_2015_Kaggle')
rm(list=ls()); gc()
library(xgboost);library(pROC);library(doMC)
load('data/6_train_validation_test_center_scale_no_dummy.RData');ls()
# load('data/6_train_validation_test_center_scale_no_dummy_2.RData');ls()
head(train)
dim(train)
dtrain <- xgb.DMatrix(data = train[,2:46], label=train$flag_class)
dtest <- xgb.DMatrix(data = validation[,2:46], label=test$flag_class)
install.packages('xgboost')
setwd('/Users/ivanliu/Google Drive/Melbourne Datathon/Melbourne_Datathon_2015_Kaggle')
install.packages("devtools")
devtools::install_github("JohnLangford/vowpal_wabbit", subdir = "R/r.vw")
library(pROC);require(data.table);library(r.vw)
load('data/Ivan_Train_Test_Scale_Center_20151121.RData');ls()
head(train)
dim(train)
feat <- colnames(total)[3:46]
feat
train_dt = data.table::setDT(train[,feat])
train$flag_class <- ifelse(train$flag_class == 'Y', 1, -1)
feat <- colnames(total)[3:46]
feat <- colnames(total)[c(3:46,48)]
train_dt = data.table::setDT(train[,feat])
target = 'flag_class'
data_types = get_feature_type(dt[, setdiff(names(dt), target), with=F], threshold = 50)
# Function used to select variables for each namespace
get_feature_type <- function(X, threshold = 50, verbose = FALSE) {
q_levels <- function (x)
{
if (data.table::is.data.table(x)) {
unlist(x[, lapply(.SD, function(x) length(unique(x)))])
}
else {
apply(x, 2, function(x) length(unique(x)))
}
}
lvs = q_levels(X)
fact_vars = names(lvs[lvs < threshold])
num_vars = names(lvs[lvs >= threshold])
if (verbose) {
print(data.frame(lvs))
}
list(fact_vars = fact_vars, num_vars = num_vars)
}
data_types = get_feature_type(dt[, setdiff(names(dt), target), with=F], threshold = 50)
data_types = get_feature_type(train_dt[, setdiff(names(train_dt), target), with=F], threshold = 50)
data_types = get_feature_type(train_dt[, setdiff(names(train_dt), target), with=F], threshold = 50)
data_types
str(train_dt)
